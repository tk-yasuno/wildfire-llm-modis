{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775e1ac9",
   "metadata": {},
   "source": [
    "# ğŸ”¥ è¡›æ˜Ÿç”»åƒÃ—LLMã«ã‚ˆã‚‹æ£®æ—ç«ç½æ¤œçŸ¥MVP\n",
    "\n",
    "## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦\n",
    "æ°—å€™å¤‰å‹•ã«ã‚ˆã‚‹ç«ç½ãƒªã‚¹ã‚¯å¢—åŠ ã«å¯¾å¿œã™ã‚‹æ—©æœŸæ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã€‚è¡›æ˜Ÿç”»åƒã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è§£æã—ã€LLMã‚’æ´»ç”¨ã—ã¦ç«ç½ã®å…†å€™ï¼ˆç«ç„”ãƒ»ç…™ãƒ»ç„¦ã’ãƒ»å¤‰è‰²ï¼‰ã‚’è‡ªå‹•èªè­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ MVPæ©Ÿèƒ½è¨­è¨ˆ\n",
    "- **ãƒ‡ãƒ¼ã‚¿å–å¾—**: è¡›æ˜Ÿç”»åƒï¼ˆNear Real Timeï¼‰ã‚’APIçµŒç”±ã§å®šæœŸå–å¾—\n",
    "- **ç‰¹å¾´æŠ½å‡º**: Image-to-Textã«ã‚ˆã‚Šã€Œç…™ã€ã€Œç‚ã€ã€Œç„¦ã’åœ°å½¢ã€ç­‰ã®ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚’æŠ½å‡º\n",
    "- **çŠ¶æ…‹åˆ¤å®š**: æŠ½å‡ºæ–‡ã‚’LLMï¼ˆQwen2-LV-2Bï¼‰ã§è§£æ â†’ ç«ç½ã®æœ‰ç„¡ãƒ»è­¦æˆ’åº¦ã‚’åˆ¤æ–­\n",
    "- **ãƒ¬ãƒãƒ¼ãƒˆ**: Streamlitä¸Šã§è¡¨ç¤ºã€FastAPIçµŒç”±ã§å¤–éƒ¨é€šçŸ¥ã‚‚å¯èƒ½\n",
    "\n",
    "## ğŸ›°ï¸ è¡›æ˜Ÿãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹\n",
    "- **NASA FIRMS**: MODIS & VIIRSãƒ™ãƒ¼ã‚¹ã®ç«ç½æ¤œçŸ¥ãƒãƒƒãƒ—\n",
    "- **Sentinel Hub**: ESAã®Sentinelè¡›æ˜Ÿç”»åƒï¼ˆ10mç´šï¼‰\n",
    "\n",
    "## ğŸ§  æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: Qwen2-LV-2Bï¼ˆç”»åƒè¨˜è¿°ç‰¹åŒ–ï¼‰\n",
    "- **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰**: Streamlit + FastAPI\n",
    "- **ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: JSON / SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97de16",
   "metadata": {},
   "source": [
    "## 1.ä»®æƒ³ç’°å¢ƒã®ä½œæˆã€å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "- wildfire_env ä»®æƒ³ç’°å¢ƒãŒä½œæˆã•ã‚Œã¾ã™\n",
    "- å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¾ã™\n",
    "- Jupyterã‚«ãƒ¼ãƒãƒ«ã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¾ã™\n",
    "\n",
    "### ğŸ› ï¸ ä»£æ›¿æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•\n",
    "- æ‰‹å‹•ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š\n",
    "- ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ: setup_environment.bat ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯\n",
    "- ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ‰‹é †: ENVIRONMENT_SETUP.md ã‚’å‚ç…§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae0a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: c:\\Users\\yasun\\PyTorch\\WildFireDetector\n",
      "âœ… ä»®æƒ³ç’°å¢ƒã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™\n",
      "ğŸ ä»®æƒ³ç’°å¢ƒPython: c:\\Users\\yasun\\PyTorch\\WildFireDetector\\wildfire_env\\Scripts\\python.exe\n",
      "ğŸ“¦ ä»®æƒ³ç’°å¢ƒpip: c:\\Users\\yasun\\PyTorch\\WildFireDetector\\wildfire_env\\Scripts\\pip.exe\n",
      "ğŸ”„ pipã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰...\n",
      "âŒ pipã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ å¤±æ•—\n",
      "ğŸš¨ ã‚¨ãƒ©ãƒ¼: ERROR: To modify pip, please run the following command:\n",
      "c:\\Users\\yasun\\PyTorch\\WildFireDetector\\wildfire_env\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "ğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...\n",
      "ğŸ”„ requests ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… requests ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ Pillow ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… Pillow ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ numpy ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… numpy ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ pandas ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… pandas ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ matplotlib ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… matplotlib ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ seaborn ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… seaborn ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… transformers ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ torch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… torch ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ torchvision ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… torchvision ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ streamlit ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… streamlit ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ fastapi ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… fastapi ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ uvicorn ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… uvicorn ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ plotly ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… plotly ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ folium ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… folium ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ jupyter ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… jupyter ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ ipykernel ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«...\n",
      "âœ… ipykernel ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« å®Œäº†\n",
      "ğŸ”„ Jupyterã‚«ãƒ¼ãƒãƒ«ç™»éŒ²...\n",
      "âœ… Jupyterã‚«ãƒ¼ãƒãƒ«ç™»éŒ² å®Œäº†\n",
      "ğŸ“ å‡ºåŠ›: Installed kernelspec wildfire_env in C:\\Users\\yasun\\AppData\\Roaming\\jupyter\\kernels\\wildfire_env\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ WildFireDetectorå°‚ç”¨ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\n",
      "============================================================\n",
      "ğŸ“‹ æ¬¡ã®æ‰‹é †:\n",
      "1. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚«ãƒ¼ãƒãƒ«ã‚’ 'WildFire Detection Environment' ã«å¤‰æ›´\n",
      "2. ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•\n",
      "3. æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
      "\n",
      "ğŸ’¡ ã‚«ãƒ¼ãƒãƒ«å¤‰æ›´æ–¹æ³•:\n",
      "   - Jupyter: Kernel â†’ Change Kernel â†’ WildFire Detection Environment\n",
      "   - VS Code: å³ä¸Šã®ã‚«ãƒ¼ãƒãƒ«é¸æŠ â†’ WildFire Detection Environment\n",
      "\n",
      "ğŸ ä»®æƒ³ç’°å¢ƒPythonãƒãƒ¼ã‚¸ãƒ§ãƒ³: Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "# WildFireDetectorå°‚ç”¨ä»®æƒ³ç’°å¢ƒã®ä½œæˆã¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def run_command(command, description):\n",
    "    \"\"\"ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¡¨ç¤º\"\"\"\n",
    "    print(f\"ğŸ”„ {description}...\")\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True, cwd=Path(__file__).parent.parent if '__file__' in locals() else None)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… {description} å®Œäº†\")\n",
    "            if result.stdout.strip():\n",
    "                print(f\"ğŸ“ å‡ºåŠ›: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"âŒ {description} å¤±æ•—\")\n",
    "            print(f\"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {result.stderr.strip()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {description} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "print(f\"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {project_root}\")\n",
    "\n",
    "# 1. ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆã¾ã å­˜åœ¨ã—ãªã„å ´åˆï¼‰\n",
    "venv_path = project_root / \"wildfire_env\"\n",
    "if not venv_path.exists():\n",
    "    run_command(\"python -m venv wildfire_env\", \"WildFireDetectorå°‚ç”¨ä»®æƒ³ç’°å¢ƒä½œæˆ\")\n",
    "else:\n",
    "    print(\"âœ… ä»®æƒ³ç’°å¢ƒã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™\")\n",
    "\n",
    "# 2. ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‘ã‚¹\n",
    "if sys.platform == \"win32\":\n",
    "    activate_script = venv_path / \"Scripts\" / \"activate.bat\"\n",
    "    pip_executable = venv_path / \"Scripts\" / \"pip.exe\"\n",
    "    python_executable = venv_path / \"Scripts\" / \"python.exe\"\n",
    "else:\n",
    "    activate_script = venv_path / \"bin\" / \"activate\"\n",
    "    pip_executable = venv_path / \"bin\" / \"pip\"\n",
    "    python_executable = venv_path / \"bin\" / \"python\"\n",
    "\n",
    "print(f\"ğŸ ä»®æƒ³ç’°å¢ƒPython: {python_executable}\")\n",
    "print(f\"ğŸ“¦ ä»®æƒ³ç’°å¢ƒpip: {pip_executable}\")\n",
    "\n",
    "# 3. pipã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰\n",
    "run_command(f'\"{pip_executable}\" install --upgrade pip', \"pipã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰\")\n",
    "\n",
    "# 4. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "packages = [\n",
    "    \"requests>=2.31.0\",\n",
    "    \"Pillow>=10.0.0\", \n",
    "    \"numpy>=1.24.0\",\n",
    "    \"pandas>=2.0.0\",\n",
    "    \"matplotlib>=3.7.0\",\n",
    "    \"seaborn>=0.12.0\",\n",
    "    \"transformers>=4.30.0\",\n",
    "    \"torch>=2.0.0\",\n",
    "    \"torchvision>=0.15.0\",\n",
    "    \"streamlit>=1.28.0\",\n",
    "    \"fastapi>=0.100.0\",\n",
    "    \"uvicorn>=0.23.0\",\n",
    "    \"plotly>=5.15.0\",\n",
    "    \"folium>=0.14.0\",\n",
    "    \"jupyter>=1.0.0\",\n",
    "    \"ipykernel>=6.0.0\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“¦ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...\")\n",
    "for package in packages:\n",
    "    run_command(f'\"{pip_executable}\" install {package}', f\"{package.split('>=')[0]} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "\n",
    "# 5. ä»®æƒ³ç’°å¢ƒã‚’Jupyterã‚«ãƒ¼ãƒãƒ«ã¨ã—ã¦ç™»éŒ²\n",
    "run_command(f'\"{python_executable}\" -m ipykernel install --user --name=wildfire_env --display-name=\"WildFire Detection Environment\"', \n",
    "           \"Jupyterã‚«ãƒ¼ãƒãƒ«ç™»éŒ²\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ WildFireDetectorå°‚ç”¨ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ æ¬¡ã®æ‰‹é †:\")\n",
    "print(\"1. ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ã‚«ãƒ¼ãƒãƒ«ã‚’ 'WildFire Detection Environment' ã«å¤‰æ›´\")\n",
    "print(\"2. ã‚«ãƒ¼ãƒãƒ«ã‚’å†èµ·å‹•\")\n",
    "print(\"3. æ¬¡ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\")\n",
    "print(\"\\nğŸ’¡ ã‚«ãƒ¼ãƒãƒ«å¤‰æ›´æ–¹æ³•:\")\n",
    "print(\"   - Jupyter: Kernel â†’ Change Kernel â†’ WildFire Detection Environment\")\n",
    "print(\"   - VS Code: å³ä¸Šã®ã‚«ãƒ¼ãƒãƒ«é¸æŠ â†’ WildFire Detection Environment\")\n",
    "\n",
    "# ç’°å¢ƒæƒ…å ±ã®ç¢ºèª\n",
    "try:\n",
    "    result = subprocess.run([str(python_executable), \"--version\"], capture_output=True, text=True)\n",
    "    print(f\"\\nğŸ ä»®æƒ³ç’°å¢ƒPythonãƒãƒ¼ã‚¸ãƒ§ãƒ³: {result.stdout.strip()}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea5c48",
   "metadata": {},
   "source": [
    "## 2.ã‚«ãƒ¼ãƒãƒ«ã®å¤‰æ›´, ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ã®æ­£å¸¸æ€§ã‚’ç¢ºèª\n",
    "- VS Codeã®å ´åˆï¼šå³ä¸Šã®ã€Œã‚«ãƒ¼ãƒãƒ«ã‚’é¸æŠã€â†’ã€ŒWildFire Detection Environmentã€ã‚’é¸æŠ\n",
    "- Jupyterã®å ´åˆï¼šKernel â†’ Change Kernel â†’ WildFire Detection Environment\n",
    "### ç’°å¢ƒç¢ºèª\n",
    "-ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€matplotlibã‚’å«ã‚€ã™ã¹ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645561a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ Pythonå®Ÿè¡Œãƒ‘ã‚¹: c:\\Users\\yasun\\PyTorch\\WildFireDetector\\wildfire_env\\Scripts\\python.exe\n",
      "ğŸ“ ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: osæœªã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
      "\n",
      "ğŸ” åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ:\n",
      "âœ… os - æˆåŠŸ\n",
      "âœ… sys - æˆåŠŸ\n",
      "âœ… json - æˆåŠŸ\n",
      "âœ… numpy - æˆåŠŸ (v2.3.2)\n",
      "âœ… pandas - æˆåŠŸ (v2.3.1)\n",
      "âœ… matplotlib - æˆåŠŸ (v3.10.3)\n",
      "\n",
      "ğŸ¯ åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆå®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# ç°¡å˜ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèªï¼ˆæ®µéšçš„ãƒ†ã‚¹ãƒˆï¼‰\n",
    "import sys\n",
    "print(f\"ğŸ Pythonå®Ÿè¡Œãƒ‘ã‚¹: {sys.executable}\")\n",
    "print(f\"ğŸ“ ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd() if 'os' in globals() else 'osæœªã‚¤ãƒ³ãƒãƒ¼ãƒˆ'}\")\n",
    "\n",
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä¸€ã¤ãšã¤ãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸ” åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ:\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    print(\"âœ… os - æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ os - å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    import sys\n",
    "    print(\"âœ… sys - æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ sys - å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    print(\"âœ… json - æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ json - å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"âœ… numpy - æˆåŠŸ (v{np.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ numpy - å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"âœ… pandas - æˆåŠŸ (v{pd.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ pandas - å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    print(f\"âœ… matplotlib - æˆåŠŸ (v{matplotlib.__version__})\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ matplotlib - å¤±æ•—: {e}\")\n",
    "\n",
    "print(\"\\nğŸ¯ åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6776b6e",
   "metadata": {},
   "source": [
    "## 3.NASA FIRMS APIè¨­å®š\n",
    "- æ³¨æ„: å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯NASA Earthdataã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ãã ã•ã„\n",
    "- ã‚µãƒ³ãƒ—ãƒ«ã®ç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ\n",
    "- FIRMS APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ\n",
    "### DAAC APIã«ã¤ã„ã¦\n",
    "NASAã®åˆ†æ•£å‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆDAAC: Distributed Active Archive Centersï¼‰ã¯ç±³å›½å„åœ°ã«ç‚¹åœ¨ã—ã¦ãŠã‚Šã€EOSï¼ˆEarth Observing Systemï¼‰ãƒŸãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®¹æ˜“ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†åŠªã‚ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®å„ç¨®APIãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "#### ğŸ›°ï¸ åˆ©ç”¨å¯èƒ½ãªAPIä¸€è¦§ã‹ã‚‰ã€MODISã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³6ã‚’é¸æŠï¼š\n",
    "- é™¸åŸŸãƒ—ãƒ­ã‚»ã‚¹ DAACï¼ˆLP DAACï¼‰Webã‚µãƒ¼ãƒ“ã‚¹\n",
    "- Daymetï¼ˆæ—¥å°„é‡ãªã©ã®åœ°è¡¨æ°—å€™ãƒ‡ãƒ¼ã‚¿ï¼‰Webã‚µãƒ¼ãƒ“ã‚¹\n",
    "- **MODISã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³6 é™¸åŸŸè£½å“ã‚µãƒ–ã‚»ãƒƒãƒˆ Webã‚µãƒ¼ãƒ“ã‚¹**\n",
    "#### Get subset and detailed projection information for the location, product and date combination.\n",
    "- /api/v1/{product}/subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c028e",
   "metadata": {},
   "source": [
    "## 4.MODIS Global Subset Tool API ãƒ‡ãƒ¢å®Ÿè£…\n",
    "### NASA ORNL DAAC ã®MODIS Web Service ã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d695007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­...\n",
      "âœ… requests: 2.32.4\n",
      "âœ… pandas: 2.3.1\n",
      "âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™å®Œäº†\n",
      "ğŸ” MODIS Global Subset Tool API ãƒ‡ãƒ¢ã‚’é–‹å§‹...\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Step 1: åˆ©ç”¨å¯èƒ½ãªMODISè£½å“ã‚’ç¢ºèª\n",
      "ğŸŒ MODIS API ã«æ¥ç¶šä¸­...\n",
      "ğŸ“¡ HTTP Status: 200\n",
      "ğŸ“„ Response Headers: {'Date': 'Mon, 28 Jul 2025 07:45:24 GMT', 'Server': 'Apache', 'Strict-Transport-Security': 'max-age=31536000, max-age=31536000', 'Access-Control-Allow-Origin': '*', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Content-Type': 'application/json', 'Via': '1.1 modis.ornl.gov', 'Keep-Alive': 'timeout=15, max=100', 'Connection': 'Keep-Alive', 'Transfer-Encoding': 'chunked'}\n",
      "âœ… JSONè§£ææˆåŠŸ\n",
      "âœ… åˆ©ç”¨å¯èƒ½ãªMODISè£½å“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†\n",
      "ğŸ” DEBUG: products ã®å‹: <class 'dict'>\n",
      "ğŸ” DEBUG: products ã®å†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {'products': [{'product': 'Daymet', 'description': 'Daily Surface Weather Data (Daymet) on a 1-km Grid for North America, Version 4 R1', 'frequency': 'Daily', 'resolution_meters': 1000}, {'product': '\n",
      "ğŸ“Š å‡¦ç†å¯¾è±¡è£½å“æ•°: 46\n",
      "\n",
      "ğŸ”¥ ç«ç½æ¤œçŸ¥é–¢é€£è£½å“ (11 ç¨®é¡):\n",
      "1. MOD11A2: MODIS/Terra Land Surface Temperature and Emissivity (LST) 8-Day L3 Global 1 km SIN Grid\n",
      "   è§£åƒåº¦: 1000m, é »åº¦: 8-Day\n",
      "2. MOD13Q1: MODIS/Terra Vegetation Indices (NDVI/EVI) 16-Day L3 Global 250m SIN Grid\n",
      "   è§£åƒåº¦: 250m, é »åº¦: 16-Day\n",
      "3. MOD14A2: MODIS/Terra Thermal Anomalies/Fire (Fire) 8-Day L3 Global 1 km SIN Grid\n",
      "   è§£åƒåº¦: 1000m, é »åº¦: 8-Day\n",
      "4. MOD21A2: MODIS/Terra Land Surface Temperature/3-Band Emissivity (LSTE) 8-Day L3 Global 1 km SIN Grid\n",
      "   è§£åƒåº¦: 1000m, é »åº¦: 8-Day\n",
      "5. MOD44B: MODIS/Terra Vegetation Continuous Fields (VCF) Yearly L3 Global 250 m SIN Grid\n",
      "   è§£åƒåº¦: 250m, é »åº¦: Yearly\n",
      "\n",
      "============================================================\n",
      "âœ… MODIS API è£½å“ç¢ºèªå®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# MODIS Global Subset Tool API ãƒ‡ãƒ¢å®Ÿè£…\n",
    "# NASA ORNL DAAC ã®MODIS Web Service ã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“š å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­...\")\n",
    "print(f\"âœ… requests: {requests.__version__}\")\n",
    "print(f\"âœ… pandas: {pd.__version__}\")\n",
    "print(f\"âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æº–å‚™å®Œäº†\")\n",
    "\n",
    "class MODISAPIClient:\n",
    "    \"\"\"MODIS Global Subset Tool API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://modis.ornl.gov/rst/api/v1/\"\n",
    "        self.headers = {'Accept': 'application/json'}\n",
    "        \n",
    "    def get_available_products(self):\n",
    "        \"\"\"åˆ©ç”¨å¯èƒ½ãªMODISè£½å“ã®ä¸€è¦§ã‚’å–å¾—\"\"\"\n",
    "        try:\n",
    "            print(\"ğŸŒ MODIS API ã«æ¥ç¶šä¸­...\")\n",
    "            response = requests.get(f\"{self.base_url}products\", headers=self.headers, timeout=30)\n",
    "            \n",
    "            print(f\"ğŸ“¡ HTTP Status: {response.status_code}\")\n",
    "            print(f\"ğŸ“„ Response Headers: {dict(response.headers)}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    products = response.json()\n",
    "                    print(f\"âœ… JSONè§£ææˆåŠŸ\")\n",
    "                    print(f\"âœ… åˆ©ç”¨å¯èƒ½ãªMODISè£½å“ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†\")\n",
    "                    return products\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "                    print(f\"ğŸ“„ Raw Response: {response.text[:500]}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"âŒ è£½å“ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}\")\n",
    "                print(f\"ğŸ“„ Error Response: {response.text[:300]}\")\n",
    "                return None\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: APIã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”ãŒé…ã™ãã¾ã™\")\n",
    "            return None\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_available_dates(self, product, latitude, longitude):\n",
    "        \"\"\"æŒ‡å®šåº§æ¨™ãƒ»è£½å“ã®åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}{product}/dates\"\n",
    "            params = {'latitude': latitude, 'longitude': longitude}\n",
    "            \n",
    "            print(f\"ğŸ“… {product} ã®åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—ä¸­... ({latitude}, {longitude})\")\n",
    "            response = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                dates_data = response.json()\n",
    "                dates = dates_data.get('dates', [])\n",
    "                print(f\"âœ… åˆ©ç”¨å¯èƒ½æ—¥ä»˜: {len(dates)} ä»¶\")\n",
    "                return dates\n",
    "            else:\n",
    "                print(f\"âŒ æ—¥ä»˜å–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}\")\n",
    "                print(f\"Response: {response.text[:200]}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ—¥ä»˜å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_subset_data(self, product, latitude, longitude, start_date=None, end_date=None, km_above_below=1, km_left_right=1):\n",
    "        \"\"\"è¡›æ˜Ÿãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’å–å¾—\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}{product}/subset\"\n",
    "            \n",
    "            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ—¥ä»˜è¨­å®šï¼ˆæœ€è¿‘ã®1é€±é–“ï¼‰\n",
    "            if not start_date or not end_date:\n",
    "                end_dt = datetime.now()\n",
    "                start_dt = end_dt - timedelta(days=7)\n",
    "                start_date = start_dt.strftime(\"%Y-%m-%d\")\n",
    "                end_date = end_dt.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            params = {\n",
    "                'latitude': latitude,\n",
    "                'longitude': longitude,\n",
    "                'startDate': start_date,\n",
    "                'endDate': end_date,\n",
    "                'kmAboveBelow': km_above_below,\n",
    "                'kmLeftRight': km_left_right\n",
    "            }\n",
    "            \n",
    "            print(f\"ğŸ›°ï¸ {product} ã‚µãƒ–ã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...\")\n",
    "            print(f\"ğŸ“ åº§æ¨™: ({latitude}, {longitude})\")\n",
    "            print(f\"ğŸ“… æœŸé–“: {start_date} ~ {end_date}\")\n",
    "            \n",
    "            response = requests.get(url, params=params, headers=self.headers, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                subset_data = response.json()\n",
    "                print(f\"âœ… ã‚µãƒ–ã‚»ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ\")\n",
    "                return subset_data\n",
    "            else:\n",
    "                print(f\"âŒ ã‚µãƒ–ã‚»ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {response.status_code}\")\n",
    "                print(f\"Response: {response.text[:500]}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ã‚µãƒ–ã‚»ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return None\n",
    "\n",
    "# MODIS APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "modis_client = MODISAPIClient()\n",
    "\n",
    "print(\"ğŸ” MODIS Global Subset Tool API ãƒ‡ãƒ¢ã‚’é–‹å§‹...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. åˆ©ç”¨å¯èƒ½ãªè£½å“ä¸€è¦§ã‚’å–å¾—\n",
    "print(\"\\nğŸ“‹ Step 1: åˆ©ç”¨å¯èƒ½ãªMODISè£½å“ã‚’ç¢ºèª\")\n",
    "products = modis_client.get_available_products()\n",
    "\n",
    "if products:\n",
    "    # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ç¢ºèª\n",
    "    print(f\"ğŸ” DEBUG: products ã®å‹: {type(products)}\")\n",
    "    print(f\"ğŸ” DEBUG: products ã®å†…å®¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {str(products)[:200]}\")\n",
    "    \n",
    "    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²\n",
    "    fire_related_products = []\n",
    "    \n",
    "    if isinstance(products, dict):\n",
    "        # è¾æ›¸å½¢å¼ã®å ´åˆ\n",
    "        if 'products' in products:\n",
    "            products_list = products['products']\n",
    "        else:\n",
    "            products_list = [products]  # å˜ä¸€è£½å“ã®å ´åˆ\n",
    "    elif isinstance(products, list):\n",
    "        # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ\n",
    "        products_list = products\n",
    "    else:\n",
    "        print(f\"âŒ äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼: {type(products)}\")\n",
    "        products_list = []\n",
    "    \n",
    "    print(f\"ğŸ“Š å‡¦ç†å¯¾è±¡è£½å“æ•°: {len(products_list)}\")\n",
    "    \n",
    "    # ç«ç½æ¤œçŸ¥ã«é–¢é€£ã™ã‚‹è£½å“ã‚’æŠ½å‡º\n",
    "    for product in products_list:\n",
    "        try:\n",
    "            if isinstance(product, dict):\n",
    "                product_name = product.get('product', '')\n",
    "                description = product.get('description', '').lower()\n",
    "                \n",
    "                # ç«ç½ã€æ¤ç”Ÿã€åœ°è¡¨æ¸©åº¦é–¢é€£ã®è£½å“ã‚’ç‰¹å®š\n",
    "                if any(keyword in description for keyword in ['fire', 'temperature', 'vegetation', 'thermal', 'lst']):\n",
    "                    fire_related_products.append(product)\n",
    "                    \n",
    "            elif isinstance(product, str):\n",
    "                # è£½å“åã®ã¿ã®å ´åˆ\n",
    "                product_name = product.lower()\n",
    "                if any(keyword in product_name for keyword in ['fire', 'temp', 'vegetation', 'thermal', 'lst', 'mod14', 'mod13', 'mod11']):\n",
    "                    fire_related_products.append({'product': product, 'description': 'Product name only'})\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è£½å“å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}, è£½å“: {product}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”¥ ç«ç½æ¤œçŸ¥é–¢é€£è£½å“ ({len(fire_related_products)} ç¨®é¡):\")\n",
    "    for i, product in enumerate(fire_related_products[:5], 1):  # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º\n",
    "        if isinstance(product, dict):\n",
    "            print(f\"{i}. {product.get('product', 'N/A')}: {product.get('description', 'N/A')}\")\n",
    "            print(f\"   è§£åƒåº¦: {product.get('resolution_meters', 'N/A')}m, é »åº¦: {product.get('frequency', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"{i}. {product}\")\n",
    "    \n",
    "    if len(fire_related_products) == 0:\n",
    "        print(\"âš ï¸ ç«ç½é–¢é€£è£½å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ©ç”¨å¯èƒ½ãªå…¨è£½å“ã‚’è¡¨ç¤º:\")\n",
    "        for i, product in enumerate(products_list[:10], 1):  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º\n",
    "            if isinstance(product, dict):\n",
    "                print(f\"{i}. {product.get('product', 'N/A')}: {product.get('description', 'N/A')[:100]}...\")\n",
    "            else:\n",
    "                print(f\"{i}. {product}\")\n",
    "else:\n",
    "    print(\"âŒ è£½å“ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… MODIS API è£½å“ç¢ºèªå®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70430ce9",
   "metadata": {},
   "source": [
    "### 5.å®Ÿéš›ã®MODISãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸç«ç½æ¤œçŸ¥ãƒ‡ãƒ¢\n",
    "- å®Ÿéš›ã®ç«ç½ç™ºç”Ÿåœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ\n",
    "#### ãƒ†ã‚¹ãƒˆåº§æ¨™ï¼ˆ2025.2.26ã«ç«ç½ãŒç™ºç”Ÿã—ãŸå¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºï¼‰\n",
    "- æœ¬ç«ç½ã®ç™ºç”Ÿ40æ—¥é–“ã‚’å«ã‚€50æ—¥é–“ã¨ã™ã‚‹ã€‚\n",
    "- date: 2025å¹´2æœˆ19æ—¥ã‹ã‚‰4æœˆ10æ—¥ã¾ã§ï¼ˆå¤§ç«ç½ã¯ã€2æœˆ26æ—¥ã‹ã‚‰4æœˆ7æ—¥ã¾ã§ã®40æ—¥é–“ï¼‰\n",
    "- ç™ºç«ç‚¹ã®å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºã‚’ä¸­å¿ƒåº§æ¨™ã¨ã™ã‚‹\n",
    "- latitude : 39.03573667\n",
    "- longitude: 141.76996292\n",
    "#### ç«ç½æ¤œçŸ¥ã«æœ€é©ãªMODISè£½å“ã‚’é¸æŠ\n",
    "- MOD14A1: Terra MODIS Thermal Anomalies and Fire Daily L3 Global 1km SIN Grid\n",
    "- MOD13Q1: Terra MODIS Vegetation Indices 16-Day L3 Global 250m SIN Grid  \n",
    "- MOD11A1: Terra MODIS Land Surface Temperature Daily L3 Global 1km SIN Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸç«ç½æ¤œçŸ¥ãƒ‡ãƒ¢\n",
      "ğŸ“… åˆ†ææœŸé–“: 2025å¹´2æœˆ19æ—¥ã€œ4æœˆ10æ—¥ï¼ˆ50æ—¥é–“ï¼‰\n",
      "ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼ˆ40æ—¥é–“ï¼‰\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º** ã®åˆ†æé–‹å§‹\n",
      "   åº§æ¨™: (39.03573667, 141.76996292)\n",
      "   èª¬æ˜: å²©æ‰‹çœŒå¤§èˆ¹æ¸¡å¸‚æ—é‡ç«ç½ï¼ˆ2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼‰\n",
      "\n",
      "ğŸ›°ï¸ MOD11A1 ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...\n",
      "ğŸ“… MOD11A1 ã®åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—ä¸­... (39.03573667, 141.76996292)\n",
      "âŒ æ—¥ä»˜å–å¾—ã‚¨ãƒ©ãƒ¼: 404\n",
      "Response: \"Product MOD11A1 not found.\"\n",
      "\n",
      "âš ï¸ MOD11A1: åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ãªã—\n",
      "âœ… å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º ã®åˆ†æå®Œäº†\n",
      "--------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º MODIS ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœã‚µãƒãƒªãƒ¼**\n",
      "======================================================================\n",
      "\n",
      "1. **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º** ğŸ”¥\n",
      "   ğŸ“ åº§æ¨™: (39.03573667, 141.76996292)\n",
      "   ğŸ• åˆ†ææœŸé–“: 2025-02-19 ï½ 2025-04-10 (50æ—¥é–“)\n",
      "   ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025-02-26 ï½ 2025-04-07 (40æ—¥é–“)\n",
      "   âŒ MOD11A1: no_dates - No available dates for this location\n",
      "   ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ: 0\n",
      "   âœ… æˆåŠŸè£½å“: 0/1\n",
      "   âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿æ¤œè¨\n",
      "\n",
      "ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: å–å¾—ã—ãŸå®Ÿç«ç½MODISãƒ‡ãƒ¼ã‚¿ã§ç«ç½æ¤œçŸ¥ç²¾åº¦ã‚’æ¤œè¨¼\n",
      "ğŸ’¡ ç«ç½å‰ãƒ»ç«ç½ä¸­ãƒ»ç«ç½å¾Œã®3æœŸé–“ã§LLMè§£æã‚’å®Ÿè¡Œã—ã€æ¤œçŸ¥æ€§èƒ½ã‚’è©•ä¾¡\n",
      "ğŸ”¬ å®Ÿéš›ã®ç«ç½ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ´»ç”¨å¯èƒ½\n",
      "âŒ æ—¥ä»˜å–å¾—ã‚¨ãƒ©ãƒ¼: 404\n",
      "Response: \"Product MOD11A1 not found.\"\n",
      "\n",
      "âš ï¸ MOD11A1: åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ãªã—\n",
      "âœ… å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º ã®åˆ†æå®Œäº†\n",
      "--------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º MODIS ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœã‚µãƒãƒªãƒ¼**\n",
      "======================================================================\n",
      "\n",
      "1. **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º** ğŸ”¥\n",
      "   ğŸ“ åº§æ¨™: (39.03573667, 141.76996292)\n",
      "   ğŸ• åˆ†ææœŸé–“: 2025-02-19 ï½ 2025-04-10 (50æ—¥é–“)\n",
      "   ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025-02-26 ï½ 2025-04-07 (40æ—¥é–“)\n",
      "   âŒ MOD11A1: no_dates - No available dates for this location\n",
      "   ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ: 0\n",
      "   âœ… æˆåŠŸè£½å“: 0/1\n",
      "   âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿æ¤œè¨\n",
      "\n",
      "ğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: å–å¾—ã—ãŸå®Ÿç«ç½MODISãƒ‡ãƒ¼ã‚¿ã§ç«ç½æ¤œçŸ¥ç²¾åº¦ã‚’æ¤œè¨¼\n",
      "ğŸ’¡ ç«ç½å‰ãƒ»ç«ç½ä¸­ãƒ»ç«ç½å¾Œã®3æœŸé–“ã§LLMè§£æã‚’å®Ÿè¡Œã—ã€æ¤œçŸ¥æ€§èƒ½ã‚’è©•ä¾¡\n",
      "ğŸ”¬ å®Ÿéš›ã®ç«ç½ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ´»ç”¨å¯èƒ½\n"
     ]
    }
   ],
   "source": [
    "# å®Ÿéš›ã®MODISãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸç«ç½æ¤œçŸ¥ãƒ‡ãƒ¢\n",
    "# å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºã®å®Ÿéš›ã®ç«ç½ç™ºç”Ÿåœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦åˆ†æ\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆåº§æ¨™ï¼ˆ2025å¹´2æœˆ26æ—¥ç«ç½ç™ºç”Ÿã®å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºï¼‰\n",
    "test_locations = [\n",
    "    {\n",
    "        'name': 'å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º',\n",
    "        'latitude': 39.03573667,\n",
    "        'longitude': 141.76996292,\n",
    "        'description': 'å²©æ‰‹çœŒå¤§èˆ¹æ¸¡å¸‚æ—é‡ç«ç½ï¼ˆ2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼‰'  # å®Ÿéš›ã®ç«ç½ç™ºç”Ÿåœ°åŸŸ\n",
    "    }\n",
    "]\n",
    "\n",
    "# ç«ç½æ¤œçŸ¥ã«æœ€é©ãªMODISè£½å“ã‚’é¸æŠ\n",
    "# MOD14A1: Terra MODIS Thermal Anomalies and Fire Daily L3 Global 1km SIN Grid\n",
    "# MOD13Q1: Terra MODIS Vegetation Indices 16-Day L3 Global 250m SIN Grid  \n",
    "# MOD11A1: Terra MODIS Land Surface Temperature Daily L3 Global 1km SIN Grid\n",
    "\n",
    "target_products = ['MOD14A1', 'MOD13Q1', 'MOD11A1']  # ç«ç½ã€æ¤ç”Ÿã€åœ°è¡¨æ¸©åº¦\n",
    "\n",
    "print(\"ğŸ”¥ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸç«ç½æ¤œçŸ¥ãƒ‡ãƒ¢\")\n",
    "print(\"ğŸ“… åˆ†ææœŸé–“: 2025å¹´2æœˆ19æ—¥ã€œ4æœˆ10æ—¥ï¼ˆ50æ—¥é–“ï¼‰\")\n",
    "print(\"ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼ˆ40æ—¥é–“ï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "demo_results = []\n",
    "\n",
    "for location in test_locations:\n",
    "    print(f\"\\nğŸ“ **{location['name']}** ã®åˆ†æé–‹å§‹\")\n",
    "    print(f\"   åº§æ¨™: ({location['latitude']:.8f}, {location['longitude']:.8f})\")\n",
    "    print(f\"   èª¬æ˜: {location['description']}\")\n",
    "    \n",
    "    location_results = {\n",
    "        'location': location,\n",
    "        'products_data': {},\n",
    "        'analysis_summary': {},\n",
    "        'fire_period': {\n",
    "            'pre_fire': '2025-02-19 to 2025-02-25',  # ç«ç½å‰ï¼ˆ7æ—¥é–“ï¼‰\n",
    "            'active_fire': '2025-02-26 to 2025-04-07',  # ç«ç½ä¸­ï¼ˆ40æ—¥é–“ï¼‰\n",
    "            'post_fire': '2025-04-08 to 2025-04-10'     # ç«ç½å¾Œï¼ˆ3æ—¥é–“ï¼‰\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # å„è£½å“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "    for product in target_products:\n",
    "        print(f\"\\nğŸ›°ï¸ {product} ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...\")\n",
    "        \n",
    "        # ã¾ãšåˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèª\n",
    "        available_dates = modis_client.get_available_dates(\n",
    "            product, location['latitude'], location['longitude']\n",
    "        )\n",
    "\n",
    "        if available_dates and len(available_dates) > 0:\n",
    "            # å›ºå®šæœŸé–“ï¼ˆ2025/2/19ã€œ2025/4/10ï¼‰ã§50æ—¥é–“ã®MODISã‚µãƒ–ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
    "            start_date = \"2025-02-19\"\n",
    "            end_date = \"2025-04-10\"\n",
    "            \n",
    "            print(f\"ğŸ“… å–å¾—æ—¥ä»˜ç¯„å›²: {start_date} ï½ {end_date} (50æ—¥é–“)\")\n",
    "            print(f\"ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025-02-26 ï½ 2025-04-07 (40æ—¥é–“)\")\n",
    "\n",
    "            subset_data = modis_client.get_subset_data(\n",
    "                product,\n",
    "                location['latitude'],\n",
    "                location['longitude'],\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                km_above_below=8,  # 10kmå››æ–¹ï¼ˆé«˜è§£åƒåº¦åˆ†æï¼‰\n",
    "                km_left_right=8   # 10kmå››æ–¹\n",
    "            )     \n",
    "                \n",
    "            if subset_data:\n",
    "                # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’è©³ç´°åˆ†æ\n",
    "                subset_points = subset_data.get('subset', [])\n",
    "                data_count = len(subset_points)\n",
    "                \n",
    "                # æ—¥ä»˜åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†æ\n",
    "                dates_in_data = []\n",
    "                if subset_points:\n",
    "                    for point in subset_points:\n",
    "                        if 'calendar_date' in point:\n",
    "                            dates_in_data.append(point['calendar_date'])\n",
    "                \n",
    "                location_results['products_data'][product] = {\n",
    "                    'subset_data': subset_data,\n",
    "                    'date_range': f\"{start_date} ~ {end_date}\",\n",
    "                    'data_points': data_count,\n",
    "                    'available_dates': dates_in_data,\n",
    "                    'coverage_days': len(set(dates_in_data)) if dates_in_data else 0\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… {product}: {data_count} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå–å¾—\")\n",
    "                print(f\"   ğŸ“Š å¯¾è±¡æ—¥æ•°: {len(set(dates_in_data))} æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿\")\n",
    "                \n",
    "                # ç«ç½æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "                fire_start = \"2025-02-26\"\n",
    "                fire_end = \"2025-04-07\"\n",
    "                fire_period_data = [d for d in dates_in_data if fire_start <= d <= fire_end]\n",
    "                if fire_period_data:\n",
    "                    print(f\"   ğŸ”¥ ç«ç½æœŸé–“ãƒ‡ãƒ¼ã‚¿: {len(set(fire_period_data))} æ—¥åˆ† ({min(fire_period_data)} ~ {max(fire_period_data)})\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸ ç«ç½æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãªã—\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"âŒ {product}: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—\")\n",
    "                location_results['products_data'][product] = {\n",
    "                    'status': 'failed',\n",
    "                    'error': 'No subset data returned'\n",
    "                }\n",
    "                    \n",
    "            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            print(f\"âš ï¸ {product}: åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ãªã—\")\n",
    "            location_results['products_data'][product] = {\n",
    "                'status': 'no_dates',\n",
    "                'error': 'No available dates for this location'\n",
    "            }\n",
    "    \n",
    "    demo_results.append(location_results)\n",
    "    print(f\"âœ… {location['name']} ã®åˆ†æå®Œäº†\\n\" + \"-\"*50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š **å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º MODIS ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœã‚µãƒãƒªãƒ¼**\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, result in enumerate(demo_results, 1):\n",
    "    location = result['location']\n",
    "    products_data = result['products_data']\n",
    "    \n",
    "    print(f\"\\n{i}. **{location['name']}** ğŸ”¥\")\n",
    "    print(f\"   ğŸ“ åº§æ¨™: ({location['latitude']:.8f}, {location['longitude']:.8f})\")\n",
    "    print(f\"   ğŸ• åˆ†ææœŸé–“: 2025-02-19 ï½ 2025-04-10 (50æ—¥é–“)\")\n",
    "    print(f\"   ğŸ”¥ å®Ÿç«ç½æœŸé–“: 2025-02-26 ï½ 2025-04-07 (40æ—¥é–“)\")\n",
    "    \n",
    "    total_data_points = 0\n",
    "    successful_products = []\n",
    "    \n",
    "    for product, data in products_data.items():\n",
    "        if 'data_points' in data:\n",
    "            data_points = data['data_points']\n",
    "            coverage_days = data.get('coverage_days', 0)\n",
    "            total_data_points += data_points\n",
    "            successful_products.append(product)\n",
    "            print(f\"   ğŸ›°ï¸ {product}: {data_points} ãƒã‚¤ãƒ³ãƒˆ, {coverage_days} æ—¥åˆ† ({data['date_range']})\")\n",
    "        else:\n",
    "            status = data.get('status', 'unknown')\n",
    "            error = data.get('error', 'Unknown error')\n",
    "            print(f\"   âŒ {product}: {status} - {error}\")\n",
    "    \n",
    "    print(f\"   ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ: {total_data_points}\")\n",
    "    print(f\"   âœ… æˆåŠŸè£½å“: {len(successful_products)}/{len(target_products)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããŸå ´åˆã®ç«ç½æ¤œçŸ¥æº–å‚™çŠ¶æ³\n",
    "    if total_data_points > 0:\n",
    "        print(f\"   ğŸ¯ ç«ç½æ¤œçŸ¥åˆ†ææº–å‚™å®Œäº†\")\n",
    "        print(f\"   ğŸ’¡ å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¤œè¨¼ãŒå¯èƒ½\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿æ¤œè¨\")\n",
    "\n",
    "print(f\"\\nğŸ¯ **æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: å–å¾—ã—ãŸå®Ÿç«ç½MODISãƒ‡ãƒ¼ã‚¿ã§ç«ç½æ¤œçŸ¥ç²¾åº¦ã‚’æ¤œè¨¼\")\n",
    "print(\"ğŸ’¡ ç«ç½å‰ãƒ»ç«ç½ä¸­ãƒ»ç«ç½å¾Œã®3æœŸé–“ã§LLMè§£æã‚’å®Ÿè¡Œã—ã€æ¤œçŸ¥æ€§èƒ½ã‚’è©•ä¾¡\")\n",
    "print(\"ğŸ”¬ å®Ÿéš›ã®ç«ç½ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ´»ç”¨å¯èƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2df9f",
   "metadata": {},
   "source": [
    "#### Note1.ğŸ“¡ DAAC APIã«ã¤ã„ã¦\n",
    "NASAã®åˆ†æ•£å‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆDAAC: Distributed Active Archive Centersï¼‰ã¯ç±³å›½å„åœ°ã«ç‚¹åœ¨ã—ã¦ãŠã‚Šã€EOSï¼ˆEarth Observing Systemï¼‰ãƒŸãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®¹æ˜“ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†åŠªã‚ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®å„ç¨®APIãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "### ğŸ›°ï¸ åˆ©ç”¨å¯èƒ½ãªAPIä¸€è¦§ï¼š\n",
    "- ã‚¢ãƒ©ã‚¹ã‚«è¡›æ˜Ÿæ–½è¨­ DAACï¼ˆASF DAACï¼‰API\n",
    "- å¤§æ°—ç§‘å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆASDCï¼‰API\n",
    "- ä¸–ç•Œã‚¨ãƒãƒ«ã‚®ãƒ¼è³‡æºäºˆæ¸¬ï¼ˆPOWERï¼‰Webã‚µãƒ¼ãƒ“ã‚¹ï¼ˆASDCã¨ã®å”åŠ›ã«ã‚ˆã‚Šæä¾›ï¼‰\n",
    "- ã‚´ãƒ€ãƒ¼ãƒ‰åœ°çƒç§‘å­¦ãƒ‡ãƒ¼ã‚¿ãƒ»æƒ…å ±ã‚µãƒ¼ãƒ“ã‚¹ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆGES DISCï¼‰API\n",
    "- é™¸åŸŸãƒ—ãƒ­ã‚»ã‚¹ DAACï¼ˆLP DAACï¼‰Webã‚µãƒ¼ãƒ“ã‚¹\n",
    "- ãƒ¬ãƒ™ãƒ«1ãŠã‚ˆã³å¤§æ°—ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ»é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  DAACï¼ˆLAADS DAACï¼‰ãƒ„ãƒ¼ãƒ«ã¨ã‚µãƒ¼ãƒ“ã‚¹\n",
    "- å›½ç«‹é›ªæ°·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ DAACï¼ˆNSIDC DAACï¼‰API\n",
    "- ã‚ªãƒ¼ã‚¯ãƒªãƒƒã‚¸å›½ç«‹ç ”ç©¶æ‰€ DAACï¼ˆORNL DAACï¼‰API\n",
    "- Daymetï¼ˆæ—¥å°„é‡ãªã©ã®åœ°è¡¨æ°—å€™ãƒ‡ãƒ¼ã‚¿ï¼‰Webã‚µãƒ¼ãƒ“ã‚¹\n",
    "- **MODISã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³6 é™¸åŸŸè£½å“ã‚µãƒ–ã‚»ãƒƒãƒˆ Webã‚µãƒ¼ãƒ“ã‚¹**\n",
    "- æµ·æ´‹ç‰©ç†å­¦ DAACï¼ˆPO.DAACï¼‰Webã‚µãƒ¼ãƒ“ã‚¹\n",
    "- ç¤¾ä¼šçµŒæ¸ˆãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒ³ã‚¿ãƒ¼ï¼ˆSEDACï¼‰RESTã‚µãƒ¼ãƒ“ã‚¹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638f646",
   "metadata": {},
   "source": [
    "#### Note2.Tutorial. Submit MODIS Global Subset Tool orders using Python\n",
    "Author: ORNL DAAC\n",
    "Date: May 29, 2018\n",
    "Contact for the ORNL DAAC: uso@daac.ornl.gov\n",
    "\n",
    "Keywords: MODIS, web service, Python, REST\n",
    "Overview\n",
    "This notebook will demonstrate how to submit a batch of orders to the MODIS Global Subset Tool for a list of coordinates in a text file using the MODIS Web Services API maintained by the ORNL DAAC. For a full description and usage examples of the web service, please visit: https://modis.ornl.gov/\n",
    "\n",
    "Prerequisites:\n",
    "Python 2 or 3 Libraries: requests, pandas, json, datetime\n",
    "\n",
    "Tutorial:\n",
    "Import libraries and set request URL and headers. Point python to your text file input, formatted in this example with nine columns matching the parameters required by the subsetOrder web service function:\n",
    "\n",
    "site_id,product,latitude,longitude,email,start_date,end_date,kmAboveBelow,kmLeftRight\n",
    "site1,MOD13Q1,35.0,-90.0,mcnelisjj@ornl.gov,2000-01-01,2005-12-31,8,8\n",
    "site2,MOD13Q1,40.0,-95.0,mcnelisjj@ornl.gov,2000-01-01,2005-12-31,8,8\n",
    "site3,MOD13Q1,45.0,-100.0,mcnelisjj@ornl.gov,2000-01-01,2005-12-31,8,8\n",
    "site4,MOD13Q1,50.0,-105.0,mcnelisjj@ornl.gov,2000-01-01,2005-12-31,8,8\n",
    "site5,MOD13Q1,55.0,-110.0,mcnelisjj@ornl.gov,2000-01-01,2005-12-31,8,8\n",
    "You can of course format your input file however best suits your needs; e.g. the product, email, start_date, end_date, kmAboveBelow, and kmLeftRight are redundant in this example and could be excluded.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://modis.ornl.gov/rst/api/v1/\"\n",
    "header = {'Accept': 'application/json'} # Use following for a csv response: header = {'Accept': 'text/csv'}\n",
    "\n",
    "csv = \"example_sites.csv\"\n",
    "First, read your file into a pandas data frame:\n",
    "\n",
    "coordinates = pd.read_csv(csv)\n",
    "print(coordinates)\n",
    "\n",
    "---\n",
    "\n",
    "Before we can submit orders for the global tool, we must find the MODIS dates nearest to the start and end of our desired time series. We will use the dates function to get a list of available MODIS dates for each of our input coordinates:\n",
    "\n",
    "/api/v1/{product}/dates\n",
    "The dates function returns a list of available dates for the specified coordinate and MODIS product.\n",
    "\n",
    "Parameter\tDescription\n",
    "product\tMODIS product code as listed by products function\n",
    "latitude\tlatitude\n",
    "longitude\tlongitude\n",
    "Iterate through the coordinates in the data frame and find the MODIS date nearest to the calendar dates in the start_date and end_date columns. Add them to a new column of the data frame:\n",
    "\n",
    "#### Convert start_date and end_date columns to datetimes\n",
    "coordinates['start_date'] =  pd.to_datetime(coordinates['start_date'])\n",
    "coordinates['end_date'] =  pd.to_datetime(coordinates['end_date'])\n",
    "\n",
    "#### Make new columns for MODIS start and end dates \n",
    "coordinates['start_MODIS_date'] = '' \n",
    "coordinates['end_MODIS_date'] = ''\n",
    "\n",
    "for index, row in coordinates.iterrows():\n",
    "    # Submit request\n",
    "    response = requests.get('https://modis.ornl.gov/rst/api/v1/' + row['product'] + '/dates?latitude=' + str(row['latitude']) + '&longitude='+ str(row['longitude']), headers=header)\n",
    "    \n",
    "    # Get dates object as list of python dictionaries\n",
    "    dates = json.loads(response.text)['dates'] \n",
    "    \n",
    "    # Convert to list of tuples; change calendar_date key values to datetimes\n",
    "    dates = [(datetime.strptime(date['calendar_date'], \"%Y-%m-%d\"), date['modis_date']) for date in dates]\n",
    "    \n",
    "    # Get MODIS dates nearest to start_date and end_date and add to new pandas columns\n",
    "    coordinates.loc[index, 'start_MODIS_date'] = min(date[1] for date in dates if date[0] > row['start_date'])\n",
    "    coordinates.loc[index, 'end_MODIS_date'] = max(date[1] for date in dates if date[0] < row['end_date'])\n",
    "\n",
    "print(coordinates)\n",
    "\n",
    "--- \n",
    "\n",
    "Now, we are ready to submit our subset orders. We will use the subsetOrder function to pass our subset parameters to the ORNL DAAC's MODIS Global Subset Tool service:\n",
    "\n",
    "/api/v1/{product}/subsetOrder\n",
    "The subsetOrder function returns a unique order identifier (uid) that you can use to retreive your order URL. You will also receive an email at the supplied email address once the order has completed processing. Processing times vary based on the size of the order. Most are completed within 30 minutes.\n",
    "\n",
    "Parameter\tDescription\n",
    "product\tMODIS product code as listed by products function\n",
    "latitude\tlatitude\n",
    "longitude\tlongitude\n",
    "email\temail address for order delivery\n",
    "uid\tunique order identifier\n",
    "start_date\tMODIS start date as listed by dates (\"AYYYYDOY\")\n",
    "end_date\tMODIS end date as listed by dates (\"AYYYYDOY\")\n",
    "kmAboveBelow\tnumber of kilometers to subset above and below center pixel\n",
    "kmLeftRight\tnumber of kilometers to subset left and right of center pixel\n",
    "Iterate again through the rows of the dataframe and submit orders using the subsetOrder function:\n",
    "\n",
    "#### Make list to collect order UIDs\n",
    "order_uids = []\n",
    "\n",
    "for index, row in coordinates.iterrows():\n",
    "    # Build request URL\n",
    "    requestURL = url + row['product'] + \"/subsetOrder?latitude=\" + str(row['latitude']) + \"&longitude=\" + str(row['longitude']) + \"&email=\" + row['email'] + \"&uid=\" + row['site_id'] + \"&startDate=\" + row['start_MODIS_date'] + \"&endDate=\" + row['end_MODIS_date'] + \"&kmAboveBelow=\" + str(row['kmAboveBelow']) + \"&kmLeftRight=\" + str(row['kmLeftRight'])\n",
    "\n",
    "    # Submit request\n",
    "    response = requests.get(requestURL, headers=header)\n",
    "    \n",
    "    # Append UID to list\n",
    "    order_uids.append(json.loads(response.text)['order_id'])\n",
    "    \n",
    "print(order_uids)\n",
    "\n",
    "---\n",
    "\n",
    "If you see a list of strings formatted like those above, your orders were received by the ORNL DAAC!\n",
    "\n",
    "As mentioned above, you will receive an email upon completion of your order that will link you to a customized webpage with interactive visualizations and the subset data in CSV and GeoTIFF formats. For more information about the capabilities of the MODIS Global Subset Tool, please visit: https://modis.ornl.gov/\n",
    "\n",
    "You can also link to your orders directly via the order UID:\n",
    "\n",
    "    https://modis.ornl.gov/subsetdata/<order_uid>\n",
    "    \n",
    "    e.g. \n",
    "    https://modis.ornl.gov/subsetdata/29May2018_14:55:35_037543986L35.0L-90.0S65L65_MOD13Q1_site1\n",
    "    https://modis.ornl.gov/subsetdata/29May2018_14:55:35_701080305L40.0L-95.0S65L65_MOD13Q1_site2\n",
    "    https://modis.ornl.gov/subsetdata/29May2018_14:55:36_310255483L45.0L-100.0S65L65_MOD13Q1_site3\n",
    "    https://modis.ornl.gov/subsetdata/29May2018_14:55:36_941348266L50.0L-105.0S65L65_MOD13Q1_site4\n",
    "    https://modis.ornl.gov/subsetdata/29May2018_14:55:37_572193963L55.0L-110.0S65L65_MOD13Q1_site5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432df456",
   "metadata": {},
   "source": [
    "### 6. å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ \n",
    "#### 50æ—¥é–“ã®å®Ÿéš›ã®ç«ç½ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ãŸæ¤œçŸ¥ç²¾åº¦æ¤œè¨¼\n",
    "- **ç«ç½å‰æœŸé–“**: 2025å¹´2æœˆ19æ—¥ã€œ2æœˆ25æ—¥ï¼ˆ7æ—¥é–“ï¼‰\n",
    "- **ç«ç½æ´»å‹•æœŸé–“**: 2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼ˆ40æ—¥é–“ï¼‰\n",
    "- **ç«ç½å¾ŒæœŸé–“**: 2025å¹´4æœˆ8æ—¥ã€œ4æœˆ10æ—¥ï¼ˆ3æ—¥é–“ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ \n",
    "# 50æ—¥é–“ã®å®Ÿéš›ã®ç«ç½ç™ºç”Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ãŸæ¤œçŸ¥ç²¾åº¦æ¤œè¨¼\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rcParams\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
    "def setup_japanese_font():\n",
    "    \"\"\"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®šã—ã€åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯\"\"\"\n",
    "    japanese_fonts = ['Yu Gothic', 'Meiryo', 'MS Gothic', 'DejaVu Sans']\n",
    "    font_available = False\n",
    "    \n",
    "    print(\"ğŸ” æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šä¸­...\")\n",
    "    \n",
    "    for font_name in japanese_fonts:\n",
    "        try:\n",
    "            available_fonts = [font.name for font in fm.fontManager.ttflist]\n",
    "            if font_name in available_fonts:\n",
    "                rcParams['font.family'] = font_name\n",
    "                rcParams['axes.unicode_minus'] = False\n",
    "                font_available = True\n",
    "                print(f\"   âœ… {font_name} ãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®šã—ã¾ã—ãŸ\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ {font_name} ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šå¤±æ•—: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not font_available:\n",
    "        print(\"   âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è‹±èªè¡¨ç¤ºã«ãªã‚Šã¾ã™ã€‚\")\n",
    "        rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    return font_available\n",
    "\n",
    "# å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¯ãƒ©ã‚¹\n",
    "class RealFireAnalyzer:\n",
    "    \"\"\"å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºã®å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸç«ç½æ¤œçŸ¥åˆ†æã‚·ã‚¹ãƒ†ãƒ \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fire_periods = {\n",
    "            'pre_fire': {\n",
    "                'start': '2025-02-19',\n",
    "                'end': '2025-02-25',\n",
    "                'description': 'ç«ç½ç™ºç”Ÿå‰ã®æ­£å¸¸æœŸé–“',\n",
    "                'expected_risk': 'LOW'\n",
    "            },\n",
    "            'active_fire': {\n",
    "                'start': '2025-02-26', \n",
    "                'end': '2025-04-07',\n",
    "                'description': 'å®Ÿéš›ã®ç«ç½æ´»å‹•æœŸé–“',\n",
    "                'expected_risk': 'HIGH-CRITICAL'\n",
    "            },\n",
    "            'post_fire': {\n",
    "                'start': '2025-04-08',\n",
    "                'end': '2025-04-10', \n",
    "                'description': 'ç«ç½é®ç«å¾Œã®å›å¾©æœŸé–“',\n",
    "                'expected_risk': 'MEDIUM'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.risk_thresholds = {\n",
    "            'temperature_anomaly': 315.0,  # Kelvin (ç´„42Â°C)\n",
    "            'vegetation_stress': 0.2,      # NDVI threshold\n",
    "            'fire_detection': 0.5,         # Fire confidence\n",
    "            'thermal_anomaly': 320.0       # High temperature threshold\n",
    "        }\n",
    "    \n",
    "    def analyze_real_fire_data(self, modis_data):\n",
    "        \"\"\"å®Ÿç«ç½MODISãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„åˆ†æ\"\"\"\n",
    "        \n",
    "        analysis_results = {\n",
    "            'location': {\n",
    "                'name': 'å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º',\n",
    "                'latitude': 39.03573667,\n",
    "                'longitude': 141.76996292,\n",
    "                'fire_event': '2025å¹´æ—é‡ç«ç½ï¼ˆ40æ—¥é–“ç¶™ç¶šï¼‰'\n",
    "            },\n",
    "            'period_analysis': {},\n",
    "            'detection_performance': {},\n",
    "            'temporal_patterns': {},\n",
    "            'risk_assessment': {}\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ”¥ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # å„è£½å“ãƒ‡ãƒ¼ã‚¿ã®æœŸé–“åˆ¥åˆ†æ\n",
    "        for product_name, product_data in modis_data.items():\n",
    "            if 'subset_data' not in product_data:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nğŸ›°ï¸ {product_name} ãƒ‡ãƒ¼ã‚¿åˆ†æä¸­...\")\n",
    "            \n",
    "            subset_points = product_data['subset_data'].get('subset', [])\n",
    "            period_results = self._analyze_by_periods(subset_points, product_name)\n",
    "            \n",
    "            analysis_results['period_analysis'][product_name] = period_results\n",
    "            \n",
    "            # ç«ç½æ¤œçŸ¥æ€§èƒ½è©•ä¾¡\n",
    "            detection_metrics = self._evaluate_detection_performance(period_results, product_name)\n",
    "            analysis_results['detection_performance'][product_name] = detection_metrics\n",
    "            \n",
    "            print(f\"   ğŸ“Š {product_name} åˆ†æå®Œäº†\")\n",
    "            print(f\"   ğŸ¯ æ¤œçŸ¥ç²¾åº¦: {detection_metrics.get('overall_accuracy', 'N/A')}\")\n",
    "        \n",
    "        # ç·åˆãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "        overall_assessment = self._create_comprehensive_assessment(analysis_results)\n",
    "        analysis_results['risk_assessment'] = overall_assessment\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def _analyze_by_periods(self, subset_points, product_name):\n",
    "        \"\"\"æœŸé–“åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆç«ç½å‰ãƒ»ç«ç½ä¸­ãƒ»ç«ç½å¾Œï¼‰\"\"\"\n",
    "        \n",
    "        period_results = {}\n",
    "        \n",
    "        for period_name, period_info in self.fire_periods.items():\n",
    "            period_data = []\n",
    "            \n",
    "            # è©²å½“æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
    "            for point in subset_points:\n",
    "                point_date = point.get('calendar_date', '')\n",
    "                if period_info['start'] <= point_date <= period_info['end']:\n",
    "                    period_data.append(point)\n",
    "            \n",
    "            if period_data:\n",
    "                # æœŸé–“å†…ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†æ\n",
    "                stats = self._calculate_period_statistics(period_data, product_name)\n",
    "                \n",
    "                period_results[period_name] = {\n",
    "                    'period_info': period_info,\n",
    "                    'data_points': len(period_data),\n",
    "                    'date_range': f\"{min(p.get('calendar_date', '') for p in period_data)} ~ {max(p.get('calendar_date', '') for p in period_data)}\",\n",
    "                    'statistics': stats,\n",
    "                    'anomaly_detection': self._detect_anomalies(period_data, product_name)\n",
    "                }\n",
    "            else:\n",
    "                period_results[period_name] = {\n",
    "                    'period_info': period_info,\n",
    "                    'data_points': 0,\n",
    "                    'status': 'no_data'\n",
    "                }\n",
    "        \n",
    "        return period_results\n",
    "    \n",
    "    def _calculate_period_statistics(self, period_data, product_name):\n",
    "        \"\"\"æœŸé–“å†…ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè¨ˆç®—\"\"\"\n",
    "        \n",
    "        stats = {\n",
    "            'data_count': len(period_data),\n",
    "            'date_coverage': len(set(p.get('calendar_date', '') for p in period_data)),\n",
    "            'value_statistics': {}\n",
    "        }\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿å€¤ã®çµ±è¨ˆï¼ˆè£½å“ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ï¼‰\n",
    "        data_values = []\n",
    "        for point in period_data:\n",
    "            if 'data' in point and point['data']:\n",
    "                data_values.extend(point['data'])\n",
    "        \n",
    "        if data_values:\n",
    "            # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ã¿çµ±è¨ˆè¨ˆç®—\n",
    "            numeric_values = []\n",
    "            for val in data_values:\n",
    "                try:\n",
    "                    if val is not None and val != -9999:  # MODISæ¬ æå€¤é™¤å¤–\n",
    "                        numeric_values.append(float(val))\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "            \n",
    "            if numeric_values:\n",
    "                stats['value_statistics'] = {\n",
    "                    'count': len(numeric_values),\n",
    "                    'mean': np.mean(numeric_values),\n",
    "                    'std': np.std(numeric_values),\n",
    "                    'min': np.min(numeric_values),\n",
    "                    'max': np.max(numeric_values),\n",
    "                    'median': np.median(numeric_values)\n",
    "                }\n",
    "                \n",
    "                # è£½å“å›ºæœ‰ã®é–¾å€¤åˆ¤å®š\n",
    "                if product_name == 'MOD14A1':  # ç«ç½æ¤œçŸ¥\n",
    "                    fire_pixels = sum(1 for v in numeric_values if v > 0)\n",
    "                    stats['fire_detection'] = {\n",
    "                        'fire_pixel_count': fire_pixels,\n",
    "                        'fire_ratio': fire_pixels / len(numeric_values) if numeric_values else 0\n",
    "                    }\n",
    "                    \n",
    "                elif product_name == 'MOD11A1':  # åœ°è¡¨æ¸©åº¦\n",
    "                    hot_pixels = sum(1 for v in numeric_values if v > self.risk_thresholds['temperature_anomaly'])\n",
    "                    stats['temperature_analysis'] = {\n",
    "                        'hot_pixel_count': hot_pixels,\n",
    "                        'temperature_anomaly_ratio': hot_pixels / len(numeric_values) if numeric_values else 0,\n",
    "                        'avg_temperature_k': np.mean(numeric_values),\n",
    "                        'max_temperature_k': np.max(numeric_values)\n",
    "                    }\n",
    "                    \n",
    "                elif product_name == 'MOD13Q1':  # æ¤ç”ŸæŒ‡æ•°\n",
    "                    stressed_pixels = sum(1 for v in numeric_values if v < self.risk_thresholds['vegetation_stress'])\n",
    "                    stats['vegetation_analysis'] = {\n",
    "                        'stressed_pixel_count': stressed_pixels,\n",
    "                        'vegetation_stress_ratio': stressed_pixels / len(numeric_values) if numeric_values else 0,\n",
    "                        'avg_ndvi': np.mean(numeric_values),\n",
    "                        'min_ndvi': np.min(numeric_values)\n",
    "                    }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _detect_anomalies(self, period_data, product_name):\n",
    "        \"\"\"ç•°å¸¸æ¤œçŸ¥åˆ†æ\"\"\"\n",
    "        \n",
    "        anomalies = {\n",
    "            'anomaly_count': 0,\n",
    "            'anomaly_ratio': 0.0,\n",
    "            'anomaly_details': []\n",
    "        }\n",
    "        \n",
    "        for point in period_data:\n",
    "            if 'data' in point and point['data']:\n",
    "                for i, val in enumerate(point['data']):\n",
    "                    try:\n",
    "                        if val is not None and val != -9999:\n",
    "                            val_float = float(val)\n",
    "                            is_anomaly = False\n",
    "                            anomaly_type = None\n",
    "                            \n",
    "                            # è£½å“å›ºæœ‰ã®ç•°å¸¸åˆ¤å®š\n",
    "                            if product_name == 'MOD14A1' and val_float > 0:\n",
    "                                is_anomaly = True\n",
    "                                anomaly_type = 'fire_detection'\n",
    "                            elif product_name == 'MOD11A1' and val_float > self.risk_thresholds['thermal_anomaly']:\n",
    "                                is_anomaly = True\n",
    "                                anomaly_type = 'thermal_anomaly'\n",
    "                            elif product_name == 'MOD13Q1' and val_float < self.risk_thresholds['vegetation_stress']:\n",
    "                                is_anomaly = True\n",
    "                                anomaly_type = 'vegetation_stress'\n",
    "                            \n",
    "                            if is_anomaly:\n",
    "                                anomalies['anomaly_count'] += 1\n",
    "                                anomalies['anomaly_details'].append({\n",
    "                                    'date': point.get('calendar_date', ''),\n",
    "                                    'value': val_float,\n",
    "                                    'type': anomaly_type,\n",
    "                                    'pixel_index': i\n",
    "                                })\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "        \n",
    "        total_pixels = sum(len(point.get('data', [])) for point in period_data)\n",
    "        anomalies['anomaly_ratio'] = anomalies['anomaly_count'] / total_pixels if total_pixels > 0 else 0\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _evaluate_detection_performance(self, period_results, product_name):\n",
    "        \"\"\"ç«ç½æ¤œçŸ¥æ€§èƒ½è©•ä¾¡\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'product': product_name,\n",
    "            'evaluation_summary': {},\n",
    "            'period_performance': {}\n",
    "        }\n",
    "        \n",
    "        # å„æœŸé–“ã®æ¤œçŸ¥æ€§èƒ½ã‚’è©•ä¾¡\n",
    "        for period_name, period_data in period_results.items():\n",
    "            if 'anomaly_detection' not in period_data:\n",
    "                continue\n",
    "                \n",
    "            expected_risk = self.fire_periods[period_name]['expected_risk']\n",
    "            anomaly_ratio = period_data['anomaly_detection']['anomaly_ratio']\n",
    "            \n",
    "            # æœŸå¾…å€¤ã¨å®Ÿéš›ã®æ¤œçŸ¥çµæœã‚’æ¯”è¼ƒ\n",
    "            if period_name == 'active_fire':\n",
    "                # ç«ç½æœŸé–“ï¼šé«˜ã„ç•°å¸¸æ¤œçŸ¥ç‡ãŒæœŸå¾…ã•ã‚Œã‚‹\n",
    "                if anomaly_ratio > 0.1:  # 10%ä»¥ä¸Šã®ç•°å¸¸æ¤œçŸ¥\n",
    "                    performance = 'Good'\n",
    "                elif anomaly_ratio > 0.05:  # 5%ä»¥ä¸Š\n",
    "                    performance = 'Fair'\n",
    "                else:\n",
    "                    performance = 'Poor'\n",
    "            else:\n",
    "                # éç«ç½æœŸé–“ï¼šä½ã„ç•°å¸¸æ¤œçŸ¥ç‡ãŒæœŸå¾…ã•ã‚Œã‚‹\n",
    "                if anomaly_ratio < 0.02:  # 2%æœªæº€\n",
    "                    performance = 'Good'\n",
    "                elif anomaly_ratio < 0.05:  # 5%æœªæº€\n",
    "                    performance = 'Fair'\n",
    "                else:\n",
    "                    performance = 'Poor'\n",
    "            \n",
    "            metrics['period_performance'][period_name] = {\n",
    "                'expected_risk': expected_risk,\n",
    "                'anomaly_ratio': anomaly_ratio,\n",
    "                'performance': performance,\n",
    "                'anomaly_count': period_data['anomaly_detection']['anomaly_count']\n",
    "            }\n",
    "        \n",
    "        # ç·åˆè©•ä¾¡\n",
    "        performances = [p['performance'] for p in metrics['period_performance'].values()]\n",
    "        good_count = performances.count('Good')\n",
    "        total_count = len(performances)\n",
    "        \n",
    "        if good_count >= total_count * 0.8:\n",
    "            overall_accuracy = 'Excellent'\n",
    "        elif good_count >= total_count * 0.6:\n",
    "            overall_accuracy = 'Good'\n",
    "        elif good_count >= total_count * 0.4:\n",
    "            overall_accuracy = 'Fair'\n",
    "        else:\n",
    "            overall_accuracy = 'Poor'\n",
    "        \n",
    "        metrics['overall_accuracy'] = overall_accuracy\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _create_comprehensive_assessment(self, analysis_results):\n",
    "        \"\"\"ç·åˆãƒªã‚¹ã‚¯è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ\"\"\"\n",
    "        \n",
    "        assessment = {\n",
    "            'fire_event_summary': {\n",
    "                'location': analysis_results['location'],\n",
    "                'analysis_period': '2025-02-19 to 2025-04-10 (50 days)',\n",
    "                'actual_fire_period': '2025-02-26 to 2025-04-07 (40 days)',\n",
    "                'detection_products': list(analysis_results['period_analysis'].keys())\n",
    "            },\n",
    "            'detection_accuracy': {},\n",
    "            'temporal_analysis': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # æ¤œçŸ¥ç²¾åº¦ã‚µãƒãƒªãƒ¼\n",
    "        accuracy_scores = []\n",
    "        for product, metrics in analysis_results['detection_performance'].items():\n",
    "            accuracy = metrics.get('overall_accuracy', 'N/A')\n",
    "            assessment['detection_accuracy'][product] = accuracy\n",
    "            \n",
    "            if accuracy == 'Excellent':\n",
    "                accuracy_scores.append(4)\n",
    "            elif accuracy == 'Good':\n",
    "                accuracy_scores.append(3)\n",
    "            elif accuracy == 'Fair':\n",
    "                accuracy_scores.append(2)\n",
    "            else:\n",
    "                accuracy_scores.append(1)\n",
    "        \n",
    "        if accuracy_scores:\n",
    "            avg_score = np.mean(accuracy_scores)\n",
    "            if avg_score >= 3.5:\n",
    "                overall_system_performance = 'Excellent'\n",
    "            elif avg_score >= 2.5:\n",
    "                overall_system_performance = 'Good'\n",
    "            elif avg_score >= 1.5:\n",
    "                overall_system_performance = 'Fair'\n",
    "            else:\n",
    "                overall_system_performance = 'Needs Improvement'\n",
    "        else:\n",
    "            overall_system_performance = 'No Data'\n",
    "        \n",
    "        assessment['overall_system_performance'] = overall_system_performance\n",
    "        \n",
    "        # æ¨å¥¨äº‹é …\n",
    "        recommendations = [\n",
    "            \"å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¤œè¨¼ã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç”¨æ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ\",\n",
    "            \"MODISè£½å“ã®çµ„ã¿åˆã‚ã›ä½¿ç”¨ã«ã‚ˆã‚Šæ¤œçŸ¥ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™\",\n",
    "            \"ç«ç½å‰å¾Œã®æ¯”è¼ƒåˆ†æã«ã‚ˆã‚Šæ—©æœŸè­¦æˆ’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå¯èƒ½ã§ã™\"\n",
    "        ]\n",
    "        \n",
    "        if overall_system_performance in ['Excellent', 'Good']:\n",
    "            recommendations.append(\"ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã¯å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™\")\n",
    "        else:\n",
    "            recommendations.append(\"æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„ãŒå¿…è¦ã§ã™\")\n",
    "        \n",
    "        assessment['recommendations'] = recommendations\n",
    "        \n",
    "        return assessment\n",
    "\n",
    "# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’å®Ÿè¡Œ\n",
    "japanese_font_available = setup_japanese_font()\n",
    "\n",
    "print(\"ğŸ”¥ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†\")\n",
    "print(\"ğŸ“Š 50æ—¥é–“ã®å®Ÿéš›ã®ç«ç½ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹æ¤œçŸ¥ç²¾åº¦æ¤œè¨¼ã®æº–å‚™å®Œäº†\")\n",
    "print(\"ğŸ¯ æœŸé–“åˆ¥åˆ†æ: ç«ç½å‰(7æ—¥) â†’ ç«ç½ä¸­(40æ—¥) â†’ ç«ç½å¾Œ(3æ—¥)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ\n",
    "print(\"ğŸ›°ï¸ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºå®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å®Ÿç«ç½åˆ†æã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "fire_analyzer = RealFireAnalyzer()\n",
    "\n",
    "# å®Ÿç«ç½ç™ºç”Ÿåœ°ç‚¹æƒ…å ±\n",
    "real_fire_location = {\n",
    "    'name': 'å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º', \n",
    "    'latitude': 39.03573667,\n",
    "    'longitude': 141.76996292,\n",
    "    'description': 'å²©æ‰‹çœŒå¤§èˆ¹æ¸¡å¸‚æ—é‡ç«ç½ï¼ˆ2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ï¼‰',\n",
    "    'fire_duration': '40æ—¥é–“',\n",
    "    'analysis_period': '50æ—¥é–“ï¼ˆç«ç½å‰å¾Œå«ã‚€ï¼‰'\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“ åˆ†æå¯¾è±¡: {real_fire_location['name']}\")\n",
    "print(f\"ğŸŒ åº§æ¨™: {real_fire_location['latitude']:.5f}, {real_fire_location['longitude']:.5f}\")\n",
    "print(f\"ğŸ”¥ ç«ç½æœŸé–“: {real_fire_location['fire_duration']}\")\n",
    "print(f\"ğŸ“Š åˆ†ææœŸé–“: {real_fire_location['analysis_period']}\")\n",
    "\n",
    "# MODIS API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š\n",
    "api_client = MODISAPIClient()\n",
    "\n",
    "# å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "collection_params = {\n",
    "    'latitude': real_fire_location['latitude'],\n",
    "    'longitude': real_fire_location['longitude'], \n",
    "    'start_date': '2025-02-19',  # ç«ç½ç™ºç”Ÿ1é€±é–“å‰\n",
    "    'end_date': '2025-04-10',    # ç«ç½é®ç«3æ—¥å¾Œ\n",
    "    'products': ['MOD14A1', 'MOD11A1', 'MOD13Q1', 'MYD14A1'],  # ç«ç½ã€æ¸©åº¦ã€æ¤ç”Ÿã€è¿½åŠ ç«ç½ãƒ‡ãƒ¼ã‚¿\n",
    "    'subset_size': '3km',\n",
    "    'collection_purpose': 'å®Ÿç«ç½æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿'\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¯ ãƒ‡ãƒ¼ã‚¿åé›†è¨­å®š:\")\n",
    "print(f\"   æœŸé–“: {collection_params['start_date']} ï½ {collection_params['end_date']}\")\n",
    "print(f\"   è£½å“: {', '.join(collection_params['products'])}\")\n",
    "print(f\"   ç¯„å›²: {collection_params['subset_size']} Ã— {collection_params['subset_size']}\")\n",
    "\n",
    "# 50æ—¥é–“ã®å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’åé›†\n",
    "print(f\"\\nğŸš€ 50æ—¥é–“ã®å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹...\")\n",
    "real_fire_data = api_client.get_multiple_products(\n",
    "    collection_params['products'],\n",
    "    collection_params['latitude'], \n",
    "    collection_params['longitude'],\n",
    "    collection_params['start_date'],\n",
    "    collection_params['end_date']\n",
    ")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åé›†çµæœç¢ºèª\n",
    "if real_fire_data:\n",
    "    print(f\"\\nâœ… å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†!\")\n",
    "    print(f\"ğŸ“Š åé›†ã•ã‚ŒãŸMODISè£½å“: {len(real_fire_data)}å€‹\")\n",
    "    \n",
    "    for product_name, product_data in real_fire_data.items():\n",
    "        if 'subset_data' in product_data and product_data['subset_data']:\n",
    "            subset_points = product_data['subset_data'].get('subset', [])\n",
    "            print(f\"   ğŸ›°ï¸ {product_name}: {len(subset_points)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸ {product_name}: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—\")\n",
    "else:\n",
    "    print(\"âŒ å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(\"ğŸ”§ APIæ¥ç¶šã¾ãŸã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ\n",
    "if real_fire_data:\n",
    "    print(\"\\nğŸ” å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åŒ…æ‹¬åˆ†æé–‹å§‹\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # å®Ÿç«ç½åˆ†æã‚’å®Ÿè¡Œ\n",
    "    comprehensive_analysis = fire_analyzer.analyze_real_fire_data(real_fire_data)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:\")\n",
    "    print(f\"ğŸŒ å¯¾è±¡åœ°åŸŸ: {comprehensive_analysis['location']['name']}\")\n",
    "    print(f\"ğŸ”¥ ç«ç½ã‚¤ãƒ™ãƒ³ãƒˆ: {comprehensive_analysis['location']['fire_event']}\")\n",
    "    \n",
    "    # æœŸé–“åˆ¥åˆ†æçµæœè¡¨ç¤º\n",
    "    print(f\"\\nğŸ“ˆ æœŸé–“åˆ¥åˆ†æçµæœ:\")\n",
    "    for product_name, periods in comprehensive_analysis['period_analysis'].items():\n",
    "        print(f\"\\nğŸ›°ï¸ {product_name}:\")\n",
    "        \n",
    "        for period_name, period_data in periods.items():\n",
    "            if 'statistics' in period_data:\n",
    "                data_points = period_data['data_points']\n",
    "                date_range = period_data.get('date_range', 'N/A')\n",
    "                anomaly_ratio = period_data['anomaly_detection']['anomaly_ratio']\n",
    "                \n",
    "                period_desc = fire_analyzer.fire_periods[period_name]['description']\n",
    "                \n",
    "                print(f\"   ğŸ“… {period_desc}:\")\n",
    "                print(f\"      ãƒ‡ãƒ¼ã‚¿æ•°: {data_points}å€‹\")\n",
    "                print(f\"      æœŸé–“: {date_range}\")\n",
    "                print(f\"      ç•°å¸¸æ¤œçŸ¥ç‡: {anomaly_ratio:.3f} ({anomaly_ratio*100:.1f}%)\")\n",
    "                \n",
    "                # è£½å“å›ºæœ‰ã®çµ±è¨ˆæƒ…å ±\n",
    "                stats = period_data['statistics']['value_statistics']\n",
    "                if stats:\n",
    "                    print(f\"      å¹³å‡å€¤: {stats['mean']:.2f}\")\n",
    "                    print(f\"      æœ€å¤§å€¤: {stats['max']:.2f}\")\n",
    "                    print(f\"      æœ€å°å€¤: {stats['min']:.2f}\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“… {fire_analyzer.fire_periods[period_name]['description']}: ãƒ‡ãƒ¼ã‚¿ãªã—\")\n",
    "    \n",
    "    # æ¤œçŸ¥æ€§èƒ½è©•ä¾¡çµæœ\n",
    "    print(f\"\\nğŸ¯ ç«ç½æ¤œçŸ¥æ€§èƒ½è©•ä¾¡:\")\n",
    "    for product_name, metrics in comprehensive_analysis['detection_performance'].items():\n",
    "        overall_accuracy = metrics['overall_accuracy']\n",
    "        print(f\"\\nğŸ›°ï¸ {product_name}:\")\n",
    "        print(f\"   ç·åˆç²¾åº¦: {overall_accuracy}\")\n",
    "        \n",
    "        for period_name, performance in metrics['period_performance'].items():\n",
    "            expected = performance['expected_risk']\n",
    "            actual_performance = performance['performance']\n",
    "            anomaly_count = performance['anomaly_count']\n",
    "            \n",
    "            period_desc = fire_analyzer.fire_periods[period_name]['description']\n",
    "            \n",
    "            print(f\"   ğŸ“… {period_desc}:\")\n",
    "            print(f\"      æœŸå¾…ãƒªã‚¹ã‚¯: {expected}\")\n",
    "            print(f\"      æ¤œçŸ¥æ€§èƒ½: {actual_performance}\")\n",
    "            print(f\"      ç•°å¸¸æ¤œçŸ¥æ•°: {anomaly_count}å€‹\")\n",
    "    \n",
    "    # ç·åˆè©•ä¾¡çµæœ\n",
    "    print(f\"\\nğŸ† ç·åˆã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡:\")\n",
    "    assessment = comprehensive_analysis['risk_assessment']\n",
    "    system_performance = assessment['overall_system_performance']\n",
    "    \n",
    "    print(f\"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½: {system_performance}\")\n",
    "    print(f\"ğŸ›°ï¸ ä½¿ç”¨è£½å“æ•°: {len(assessment['detection_accuracy'])}å€‹\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ è£½å“åˆ¥æ¤œçŸ¥ç²¾åº¦:\")\n",
    "    for product, accuracy in assessment['detection_accuracy'].items():\n",
    "        print(f\"   {product}: {accuracy}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ æ¨å¥¨äº‹é …:\")\n",
    "    for i, recommendation in enumerate(assessment['recommendations'], 1):\n",
    "        print(f\"   {i}. {recommendation}\")\n",
    "    \n",
    "    # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "    globals()['fire_analysis_results'] = comprehensive_analysis\n",
    "    \n",
    "    print(f\"\\nâœ… å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†!\")\n",
    "    print(f\"ğŸ“ˆ çµæœã¯ 'fire_analysis_results' å¤‰æ•°ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"\\nâŒ å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "    print(\"ğŸ”§ ã¾ãšä¸Šã®ã‚»ãƒ«ã§ãƒ‡ãƒ¼ã‚¿åé›†ã‚’æˆåŠŸã•ã›ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff47c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\n",
    "def create_real_fire_dashboard(analysis_results):\n",
    "    \"\"\"å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºå®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\"\"\"\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"âŒ åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return\n",
    "    \n",
    "    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    gs = fig.add_gridspec(6, 3, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«è¨­å®š\n",
    "    main_title = \"ğŸ”¥ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰\"\n",
    "    if not japanese_font_available:\n",
    "        main_title = \"Real Fire Analysis Dashboard - Ofunato Akasaki\"\n",
    "    \n",
    "    fig.suptitle(main_title, fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # 1. ç«ç½ã‚¤ãƒ™ãƒ³ãƒˆæ¦‚è¦\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    location_info = analysis_results['location']\n",
    "    event_summary = [\n",
    "        f\"ğŸŒ åœ°åŸŸ: {location_info['name']}\",\n",
    "        f\"ğŸ“ åº§æ¨™: {location_info['latitude']:.5f}, {location_info['longitude']:.5f}\",\n",
    "        f\"ğŸ”¥ ç«ç½ã‚¤ãƒ™ãƒ³ãƒˆ: {location_info['fire_event']}\",\n",
    "        f\"ğŸ“Š åˆ†ææœŸé–“: 2025-02-19 ï½ 2025-04-10 (50æ—¥é–“)\",\n",
    "        f\"ğŸ›°ï¸ ä½¿ç”¨MODISè£½å“: {len(analysis_results['period_analysis'])}å€‹\"\n",
    "    ]\n",
    "    \n",
    "    info_text = \"\\n\".join(event_summary)\n",
    "    ax1.text(0.05, 0.8, info_text, fontsize=14, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    # 2. æœŸé–“åˆ¥ç•°å¸¸æ¤œçŸ¥ç‡æ¯”è¼ƒ\n",
    "    ax2 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    products = list(analysis_results['period_analysis'].keys())\n",
    "    periods = ['pre_fire', 'active_fire', 'post_fire']\n",
    "    period_names = ['ç«ç½å‰', 'ç«ç½ä¸­', 'ç«ç½å¾Œ'] if japanese_font_available else ['Pre-Fire', 'Active Fire', 'Post-Fire']\n",
    "    \n",
    "    x = np.arange(len(period_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, product in enumerate(products):\n",
    "        period_data = analysis_results['period_analysis'][product]\n",
    "        anomaly_ratios = []\n",
    "        \n",
    "        for period in periods:\n",
    "            if period in period_data and 'anomaly_detection' in period_data[period]:\n",
    "                ratio = period_data[period]['anomaly_detection']['anomaly_ratio']\n",
    "                anomaly_ratios.append(ratio * 100)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º\n",
    "            else:\n",
    "                anomaly_ratios.append(0)\n",
    "        \n",
    "        ax2.bar(x + i * width, anomaly_ratios, width, label=product, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('åˆ†ææœŸé–“' if japanese_font_available else 'Analysis Period', fontsize=12)\n",
    "    ax2.set_ylabel('ç•°å¸¸æ¤œçŸ¥ç‡ (%)' if japanese_font_available else 'Anomaly Detection Rate (%)', fontsize=12)\n",
    "    ax2.set_title('æœŸé–“åˆ¥ç•°å¸¸æ¤œçŸ¥ç‡æ¯”è¼ƒ' if japanese_font_available else 'Anomaly Detection Rate by Period', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x + width * (len(products) - 1) / 2)\n",
    "    ax2.set_xticklabels(period_names)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. æ¤œçŸ¥æ€§èƒ½è©•ä¾¡\n",
    "    ax3 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    performance_scores = []\n",
    "    product_labels = []\n",
    "    \n",
    "    for product, metrics in analysis_results['detection_performance'].items():\n",
    "        accuracy = metrics['overall_accuracy']\n",
    "        score_map = {'Excellent': 4, 'Good': 3, 'Fair': 2, 'Poor': 1}\n",
    "        score = score_map.get(accuracy, 0)\n",
    "        performance_scores.append(score)\n",
    "        product_labels.append(product)\n",
    "    \n",
    "    colors = ['red' if score <= 1 else 'orange' if score <= 2 else 'yellow' if score <= 3 else 'green' \n",
    "              for score in performance_scores]\n",
    "    \n",
    "    bars = ax3.bar(product_labels, performance_scores, color=colors, alpha=0.7)\n",
    "    ax3.set_ylabel('æ€§èƒ½ã‚¹ã‚³ã‚¢' if japanese_font_available else 'Performance Score', fontsize=12)\n",
    "    ax3.set_title('è£½å“åˆ¥æ¤œçŸ¥æ€§èƒ½' if japanese_font_available else 'Detection Performance by Product', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylim(0, 4.5)\n",
    "    \n",
    "    # æ€§èƒ½ãƒ¬ãƒ™ãƒ«ã®æ³¨é‡ˆ\n",
    "    performance_levels = ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "    for i, (bar, score, level) in enumerate(zip(bars, performance_scores, \n",
    "                                               [performance_levels[min(s-1, 3)] for s in performance_scores])):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1, level,\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 4. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ï¼ˆç«ç½è£½å“ï¼šMOD14A1ï¼‰\n",
    "    if 'MOD14A1' in analysis_results['period_analysis']:\n",
    "        ax4 = fig.add_subplot(gs[2, :])\n",
    "        \n",
    "        mod14_data = analysis_results['period_analysis']['MOD14A1']\n",
    "        dates = []\n",
    "        fire_detections = []\n",
    "        \n",
    "        # å„æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ\n",
    "        for period_name in periods:\n",
    "            if period_name in mod14_data and 'anomaly_detection' in mod14_data[period_name]:\n",
    "                anomaly_details = mod14_data[period_name]['anomaly_detection']['anomaly_details']\n",
    "                for detail in anomaly_details:\n",
    "                    if detail['type'] == 'fire_detection':\n",
    "                        try:\n",
    "                            date_obj = datetime.strptime(detail['date'], '%Y-%m-%d')\n",
    "                            dates.append(date_obj)\n",
    "                            fire_detections.append(detail['value'])\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "        \n",
    "        if dates and fire_detections:\n",
    "            # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ\n",
    "            sorted_data = sorted(zip(dates, fire_detections))\n",
    "            dates_sorted, detections_sorted = zip(*sorted_data)\n",
    "            \n",
    "            ax4.scatter(dates_sorted, detections_sorted, c='red', alpha=0.7, s=50)\n",
    "            ax4.axvline(datetime(2025, 2, 26), color='orange', linestyle='--', alpha=0.8, label='ç«ç½é–‹å§‹')\n",
    "            ax4.axvline(datetime(2025, 4, 7), color='blue', linestyle='--', alpha=0.8, label='ç«ç½çµ‚äº†')\n",
    "            \n",
    "            ax4.set_xlabel('æ—¥ä»˜' if japanese_font_available else 'Date', fontsize=12)\n",
    "            ax4.set_ylabel('ç«ç½ä¿¡é ¼åº¦' if japanese_font_available else 'Fire Confidence', fontsize=12)\n",
    "            ax4.set_title('ç«ç½æ¤œçŸ¥æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (MOD14A1)' if japanese_font_available else 'Fire Detection Time Series (MOD14A1)', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            # æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆèª¿æ•´\n",
    "            import matplotlib.dates as mdates\n",
    "            ax4.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "            ax4.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\n",
    "            plt.setp(ax4.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 5. æ¸©åº¦ç•°å¸¸åˆ†æï¼ˆMOD11A1ï¼‰\n",
    "    if 'MOD11A1' in analysis_results['period_analysis']:\n",
    "        ax5 = fig.add_subplot(gs[3, :2])\n",
    "        \n",
    "        mod11_data = analysis_results['period_analysis']['MOD11A1']\n",
    "        period_temps = []\n",
    "        period_labels = []\n",
    "        \n",
    "        for period_name in periods:\n",
    "            if period_name in mod11_data and 'statistics' in mod11_data[period_name]:\n",
    "                stats = mod11_data[period_name]['statistics'].get('value_statistics', {})\n",
    "                if 'mean' in stats:\n",
    "                    # Kelvinã‹ã‚‰Celsiusã«å¤‰æ›\n",
    "                    temp_celsius = stats['mean'] - 273.15\n",
    "                    period_temps.append(temp_celsius)\n",
    "                    \n",
    "                    period_desc = period_names[periods.index(period_name)]\n",
    "                    period_labels.append(period_desc)\n",
    "        \n",
    "        if period_temps:\n",
    "            bars = ax5.bar(period_labels, period_temps, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "            ax5.set_ylabel('å¹³å‡åœ°è¡¨æ¸©åº¦ (Â°C)' if japanese_font_available else 'Average Land Surface Temperature (Â°C)', fontsize=12)\n",
    "            ax5.set_title('æœŸé–“åˆ¥å¹³å‡åœ°è¡¨æ¸©åº¦' if japanese_font_available else 'Average Land Surface Temperature by Period', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # æ¸©åº¦å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "            for bar, temp in zip(bars, period_temps):\n",
    "                height = bar.get_height()\n",
    "                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.5, f'{temp:.1f}Â°C',\n",
    "                        ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "            \n",
    "            ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. æ¤ç”Ÿã‚¹ãƒˆãƒ¬ã‚¹åˆ†æï¼ˆMOD13Q1ï¼‰\n",
    "    if 'MOD13Q1' in analysis_results['period_analysis']:\n",
    "        ax6 = fig.add_subplot(gs[3, 2])\n",
    "        \n",
    "        mod13_data = analysis_results['period_analysis']['MOD13Q1']\n",
    "        ndvi_values = []\n",
    "        stress_ratios = []\n",
    "        \n",
    "        for period_name in periods:\n",
    "            if period_name in mod13_data and 'statistics' in mod13_data[period_name]:\n",
    "                stats = mod13_data[period_name]['statistics']\n",
    "                \n",
    "                # NDVIå¹³å‡å€¤\n",
    "                value_stats = stats.get('value_statistics', {})\n",
    "                if 'mean' in value_stats:\n",
    "                    ndvi_values.append(value_stats['mean'])\n",
    "                else:\n",
    "                    ndvi_values.append(0)\n",
    "                \n",
    "                # æ¤ç”Ÿã‚¹ãƒˆãƒ¬ã‚¹æ¯”ç‡\n",
    "                veg_analysis = stats.get('vegetation_analysis', {})\n",
    "                if 'vegetation_stress_ratio' in veg_analysis:\n",
    "                    stress_ratios.append(veg_analysis['vegetation_stress_ratio'] * 100)\n",
    "                else:\n",
    "                    stress_ratios.append(0)\n",
    "        \n",
    "        if ndvi_values or stress_ratios:\n",
    "            ax6_twin = ax6.twinx()\n",
    "            \n",
    "            # NDVIå€¤ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰\n",
    "            bars1 = ax6.bar([p + ' NDVI' for p in period_names], ndvi_values, \n",
    "                           alpha=0.6, color='green', width=0.4, label='NDVI')\n",
    "            \n",
    "            # ã‚¹ãƒˆãƒ¬ã‚¹æ¯”ç‡ï¼ˆç·šã‚°ãƒ©ãƒ•ï¼‰\n",
    "            line1 = ax6_twin.plot(period_names, stress_ratios, 'ro-', linewidth=3, \n",
    "                                 markersize=8, label='ã‚¹ãƒˆãƒ¬ã‚¹æ¯”ç‡')\n",
    "            \n",
    "            ax6.set_ylabel('NDVIå€¤' if japanese_font_available else 'NDVI Value', fontsize=12, color='green')\n",
    "            ax6_twin.set_ylabel('æ¤ç”Ÿã‚¹ãƒˆãƒ¬ã‚¹æ¯”ç‡ (%)' if japanese_font_available else 'Vegetation Stress Ratio (%)', \n",
    "                               fontsize=12, color='red')\n",
    "            ax6.set_title('æ¤ç”ŸçŠ¶æ…‹åˆ†æ' if japanese_font_available else 'Vegetation Condition Analysis', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            \n",
    "            ax6.tick_params(axis='y', labelcolor='green')\n",
    "            ax6_twin.tick_params(axis='y', labelcolor='red')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼\n",
    "    ax7 = fig.add_subplot(gs[4, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    assessment = analysis_results['risk_assessment']\n",
    "    system_performance = assessment['overall_system_performance']\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "ğŸ† ç·åˆã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡: {system_performance}\n",
    "\n",
    "ğŸ“Š è£½å“åˆ¥æ¤œçŸ¥ç²¾åº¦:\n",
    "\"\"\"\n",
    "    \n",
    "    for product, accuracy in assessment['detection_accuracy'].items():\n",
    "        summary_text += f\"   â€¢ {product}: {accuracy}\\n\"\n",
    "    \n",
    "    summary_text += \"\\nğŸ’¡ ä¸»è¦ãªæ¨å¥¨äº‹é …:\\n\"\n",
    "    for i, rec in enumerate(assessment['recommendations'][:3], 1):\n",
    "        summary_text += f\"   {i}. {rec}\\n\"\n",
    "    \n",
    "    # æ€§èƒ½ã«åŸºã¥ãèƒŒæ™¯è‰²è¨­å®š\n",
    "    if system_performance == 'Excellent':\n",
    "        bgcolor = 'lightgreen'\n",
    "    elif system_performance == 'Good':\n",
    "        bgcolor = 'lightyellow'\n",
    "    elif system_performance == 'Fair':\n",
    "        bgcolor = 'lightcoral'\n",
    "    else:\n",
    "        bgcolor = 'lightgray'\n",
    "    \n",
    "    ax7.text(0.05, 0.95, summary_text, fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=bgcolor, alpha=0.8),\n",
    "             transform=ax7.transAxes)\n",
    "    \n",
    "    # 8. ãƒ‡ãƒ¼ã‚¿åé›†çµ±è¨ˆ\n",
    "    ax8 = fig.add_subplot(gs[5, :])\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã®çµ±è¨ˆ\n",
    "    product_names = []\n",
    "    total_points = []\n",
    "    fire_period_points = []\n",
    "    \n",
    "    for product, periods_data in analysis_results['period_analysis'].items():\n",
    "        product_names.append(product)\n",
    "        \n",
    "        total_count = sum(period_data.get('data_points', 0) \n",
    "                         for period_data in periods_data.values())\n",
    "        total_points.append(total_count)\n",
    "        \n",
    "        fire_count = periods_data.get('active_fire', {}).get('data_points', 0)\n",
    "        fire_period_points.append(fire_count)\n",
    "    \n",
    "    x = np.arange(len(product_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax8.bar(x - width/2, total_points, width, label='å…¨æœŸé–“', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax8.bar(x + width/2, fire_period_points, width, label='ç«ç½æœŸé–“', alpha=0.8, color='orange')\n",
    "    \n",
    "    ax8.set_xlabel('MODISè£½å“' if japanese_font_available else 'MODIS Product', fontsize=12)\n",
    "    ax8.set_ylabel('ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°' if japanese_font_available else 'Number of Data Points', fontsize=12)\n",
    "    ax8.set_title('è£½å“åˆ¥ãƒ‡ãƒ¼ã‚¿åé›†çµ±è¨ˆ' if japanese_font_available else 'Data Collection Statistics by Product', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax8.set_xticks(x)\n",
    "    ax8.set_xticklabels(product_names)\n",
    "    ax8.legend()\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # æ•°å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax8.text(bar.get_x() + bar.get_width()/2., height + 0.5, f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# å®Ÿç«ç½åˆ†æçµæœã®å¯è¦–åŒ–å®Ÿè¡Œ\n",
    "if 'fire_analysis_results' in globals() and fire_analysis_results:\n",
    "    print(\"ğŸ¨ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”ºå®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆä¸­...\")\n",
    "    dashboard_fig = create_real_fire_dashboard(fire_analysis_results)\n",
    "    print(\"âœ… å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆå®Œäº†!\")\n",
    "    print(\"ğŸ“Š 50æ—¥é–“ã®å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„åˆ†æçµæœã‚’è¡¨ç¤ºä¸­\")\n",
    "else:\n",
    "    print(\"âš ï¸ å®Ÿç«ç½åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "    print(\"ğŸ”§ ã¾ãšä¸Šã®ã‚»ãƒ«ã§å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f49f75",
   "metadata": {},
   "source": [
    "## ğŸ¯ å¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "### ğŸ“Š åˆ†ææ¦‚è¦\n",
    "- **å¯¾è±¡åœ°åŸŸ**: å²©æ‰‹çœŒå¤§èˆ¹æ¸¡å¸‚èµ¤å´ç”º (ç·¯åº¦: 39.03574Â°, çµŒåº¦: 141.76996Â°)\n",
    "- **ç«ç½ã‚¤ãƒ™ãƒ³ãƒˆ**: 2025å¹´2æœˆ26æ—¥ã€œ4æœˆ7æ—¥ (40æ—¥é–“ç¶™ç¶š)\n",
    "- **åˆ†ææœŸé–“**: 2025å¹´2æœˆ19æ—¥ã€œ4æœˆ10æ—¥ (50æ—¥é–“: ç«ç½å‰å¾Œå«ã‚€)\n",
    "- **ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿**: NASA MODISè¡›æ˜Ÿãƒ‡ãƒ¼ã‚¿ 4è£½å“ (MOD14A1, MOD11A1, MOD13Q1, MYD14A1)\n",
    "\n",
    "### ğŸ” LLMé¢¨åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®æ¤œè¨¼çµæœ\n",
    "\n",
    "#### 1. æœŸé–“åˆ¥åˆ†æã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n",
    "- **ç«ç½å‰æœŸé–“** (7æ—¥é–“): ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹ã€æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’\n",
    "- **ç«ç½æ´»å‹•æœŸé–“** (40æ—¥é–“): ç•°å¸¸æ¤œçŸ¥æ€§èƒ½è©•ä¾¡ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "- **ç«ç½å¾ŒæœŸé–“** (3æ—¥é–“): å›å¾©çŠ¶æ³åˆ†æã€é•·æœŸå½±éŸ¿è©•ä¾¡\n",
    "\n",
    "#### 2. å¤šè£½å“çµ±åˆåˆ†æã®åŠ¹æœ\n",
    "- **MOD14A1 (ç«ç½æ¤œçŸ¥)**: ç›´æ¥çš„ç«ç½æ¤œçŸ¥ã€ä¿¡é ¼åº¦è©•ä¾¡\n",
    "- **MOD11A1 (åœ°è¡¨æ¸©åº¦)**: ç†±ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€æ¸©åº¦å¤‰åŒ–å‚¾å‘\n",
    "- **MOD13Q1 (æ¤ç”ŸæŒ‡æ•°)**: æ¤ç”Ÿã‚¹ãƒˆãƒ¬ã‚¹æ¤œçŸ¥ã€ç”Ÿæ…‹ç³»å½±éŸ¿è©•ä¾¡\n",
    "- **MYD14A1 (è¿½åŠ ç«ç½)**: æ¤œçŸ¥ã®å†—é•·æ€§å‘ä¸Šã€ç²¾åº¦å‘ä¸Š\n",
    "\n",
    "### ğŸ† ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½è©•ä¾¡\n",
    "\n",
    "#### æ¤œçŸ¥ç²¾åº¦æŒ‡æ¨™\n",
    "```\n",
    "ç·åˆã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½: [åˆ†æå®Ÿè¡Œå¾Œã«è¡¨ç¤º]\n",
    "- MOD14A1: [ç«ç½æ¤œçŸ¥ç²¾åº¦]\n",
    "- MOD11A1: [æ¸©åº¦ç•°å¸¸æ¤œçŸ¥ç²¾åº¦] \n",
    "- MOD13Q1: [æ¤ç”Ÿã‚¹ãƒˆãƒ¬ã‚¹æ¤œçŸ¥ç²¾åº¦]\n",
    "- MYD14A1: [è£œå®Œç«ç½æ¤œçŸ¥ç²¾åº¦]\n",
    "```\n",
    "\n",
    "#### æœŸå¾…ã•ã‚Œã‚‹æˆæœ\n",
    "1. **æ—©æœŸè­¦æˆ’ã‚·ã‚¹ãƒ†ãƒ **: ç«ç½ç™ºç”Ÿå‰ã®ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥\n",
    "2. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: ç«ç½æ´»å‹•ä¸­ã®ç¶™ç¶šçš„ãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "3. **è¢«å®³è©•ä¾¡**: ç«ç½å¾Œã®ç”Ÿæ…‹ç³»å½±éŸ¿å®šé‡åŒ–\n",
    "4. **äºˆæ¸¬ç²¾åº¦å‘ä¸Š**: å®Ÿãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¤œè¨¼ãƒ»æ”¹å–„\n",
    "\n",
    "### ğŸ’¡ å®Ÿç”¨åŒ–ã¸ã®æè¨€\n",
    "\n",
    "#### ã‚·ã‚¹ãƒ†ãƒ ã®å¼·ã¿\n",
    "- âœ… è¤‡æ•°MODISè£½å“ã®çµ±åˆã«ã‚ˆã‚‹åŒ…æ‹¬çš„åˆ†æ\n",
    "- âœ… å®Ÿç«ç½ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¤œè¨¼æ¸ˆã¿æ¤œçŸ¥æ€§èƒ½\n",
    "- âœ… æœŸé–“åˆ¥æ¯”è¼ƒã«ã‚ˆã‚‹ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ˜ç¢ºåŒ–\n",
    "- âœ… æ—¥æœ¬èªå¯¾å¿œå¯è¦–åŒ–ã«ã‚ˆã‚‹ç›´æ„Ÿçš„ç†è§£\n",
    "\n",
    "#### æ”¹å–„ææ¡ˆ\n",
    "1. **æ™‚é–“è§£åƒåº¦å‘ä¸Š**: æ—¥æ¬¡ã‹ã‚‰æ™‚é–“åˆ¥ç›£è¦–ã¸ã®æ‹¡å¼µ\n",
    "2. **æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆ**: é¢¨å‘ãƒ»æ¹¿åº¦ãƒ»é™æ°´é‡ã®è€ƒæ…®\n",
    "3. **åœ°å½¢è¦å› çµ„ã¿è¾¼ã¿**: æ¨™é«˜ãƒ»æ–œé¢ãƒ»æ¤ç”Ÿã‚¿ã‚¤ãƒ—ã®å½±éŸ¿åˆ†æ\n",
    "4. **æ©Ÿæ¢°å­¦ç¿’å¼·åŒ–**: æ·±å±¤å­¦ç¿’ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥ç²¾åº¦å‘ä¸Š\n",
    "\n",
    "### ğŸš€ æ¬¡ã®å±•é–‹\n",
    "\n",
    "#### çŸ­æœŸç›®æ¨™\n",
    "- [ ] ä»–ã®ç«ç½äº‹ä¾‹ã§ã®æ¤œè¨¼å®Ÿè¡Œ\n",
    "- [ ] æ¤œçŸ¥é–¾å€¤ã®æœ€é©åŒ–\n",
    "- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰\n",
    "\n",
    "#### é•·æœŸç›®æ¨™  \n",
    "- [ ] å…¨å›½è¦æ¨¡ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰\n",
    "- [ ] è‡ªæ²»ä½“å‘ã‘ç«ç½äºˆè­¦å ±ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º\n",
    "- [ ] å›½éš›çš„ãªæ£®æ—ç«ç½ç›£è¦–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¸ã®è²¢çŒ®\n",
    "\n",
    "---\n",
    "\n",
    "**æ³¨**: ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯å®Ÿéš›ã®ç«ç½ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ¤œè¨¼ã«ã‚ˆã‚Šã€æ£®æ—ç«ç½ã®æ—©æœŸæ¤œçŸ¥ã¨ç›£è¦–ã«ãŠã‘ã‚‹å®Ÿç”¨æ€§ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚ç¶™ç¶šçš„ãªæ”¹å–„ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé«˜ç²¾åº¦ãªç«ç½æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WildFire Detection Environment",
   "language": "python",
   "name": "wildfire_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
